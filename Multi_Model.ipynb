{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mNAvIBJ3QFa8"
   },
   "source": [
    "# **Importing All packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "id": "Yn6uaRjyMs9G",
    "outputId": "942d84e7-0ee0-4482-f4a0-0e3f33a25199"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, regularizers, optimizers\n",
    "from keras.layers import Dense, Add, Flatten, Input, Activation, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, Lambda, concatenate\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from numpy import load, save\n",
    "import re\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5mAxPTkiAxDd"
   },
   "source": [
    "# **Image Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Evst1ZMPSwUD"
   },
   "outputs": [],
   "source": [
    "## Reading training.csv file using pandas\n",
    "train=pd.read_csv('/content/drive/My Drive/HUSE/training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "HQ8esQgiS0Lm",
    "outputId": "ab7725ed-0539-429d-c6fe-ffe4b777f7df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>name</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5da821eb7f3e301b4504bb4a_0.jpg</td>\n",
       "      <td>Marc Jacobs Beauty  Eye-Conic Longwear Eyeshad...</td>\n",
       "      <td>beauty&lt;makeup&lt;eyeshadow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5da81cde7861c2af6a5a88cf_0.jpg</td>\n",
       "      <td>Marc Jacobs Beauty  Eye-Conic Longwear Eyeshad...</td>\n",
       "      <td>beauty&lt;makeup&lt;eyeshadow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5da8219a6504fb65da0050aa_0.jpg</td>\n",
       "      <td>Prada  Wool sweater</td>\n",
       "      <td>clothing&lt;knitwear&lt;fine knit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5da821856504fb65cea703cf_0.jpg</td>\n",
       "      <td>Prada  Printed silk-satin twill straight-leg p...</td>\n",
       "      <td>clothing&lt;pants&lt;straight leg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5da82200b25b035d1d17bec6_0.jpg</td>\n",
       "      <td>Prada  Cropped chain-trimmed printed crepe str...</td>\n",
       "      <td>clothing&lt;pants&lt;straight leg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            image  ...                      classes\n",
       "0  5da821eb7f3e301b4504bb4a_0.jpg  ...      beauty<makeup<eyeshadow\n",
       "1  5da81cde7861c2af6a5a88cf_0.jpg  ...      beauty<makeup<eyeshadow\n",
       "2  5da8219a6504fb65da0050aa_0.jpg  ...  clothing<knitwear<fine knit\n",
       "3  5da821856504fb65cea703cf_0.jpg  ...  clothing<pants<straight leg\n",
       "4  5da82200b25b035d1d17bec6_0.jpg  ...  clothing<pants<straight leg\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rExIs2ZYQIRU"
   },
   "source": [
    "There are three columns or features in training file. First column is image names of products which wee will use to read images, 2nd feature is name of the product and last feature is class or label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QI5bUp3EL9k3"
   },
   "outputs": [],
   "source": [
    "## Randomly shuffling data \n",
    "train=shuffle(train)\n",
    "train=train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "8fag-ovYL8uG",
    "outputId": "7b558a28-b3de-4dc3-8289-7d8261d9c444"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>name</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5da81e6d465e00845f490b5b_4.jpg</td>\n",
       "      <td>Acne Studios  Lavinia shearling jacket</td>\n",
       "      <td>clothing&lt;jackets&lt;casual jackets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5e497e394c033d18b291a5b7_4.jpg</td>\n",
       "      <td>AMBUSH®  Oversized tie-detailed printed cotton...</td>\n",
       "      <td>clothing&lt;tops&lt;t-shirts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5e4d39024c033d562da0e19c_1.jpg</td>\n",
       "      <td>ViX  Spring Romance shirred floral-print stret...</td>\n",
       "      <td>clothing&lt;beachwear&lt;coverups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5da81accb25b035d1d17a519_1.jpg</td>\n",
       "      <td>Balmain  Double-breasted wool-twill blazer</td>\n",
       "      <td>clothing&lt;jackets&lt;blazers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5da8218fb25b035d1d17b7e8_1.jpg</td>\n",
       "      <td>Max Mara  Kenia wool-twill wide-leg pants</td>\n",
       "      <td>clothing&lt;pants&lt;wide leg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            image  ...                          classes\n",
       "0  5da81e6d465e00845f490b5b_4.jpg  ...  clothing<jackets<casual jackets\n",
       "1  5e497e394c033d18b291a5b7_4.jpg  ...           clothing<tops<t-shirts\n",
       "2  5e4d39024c033d562da0e19c_1.jpg  ...      clothing<beachwear<coverups\n",
       "3  5da81accb25b035d1d17a519_1.jpg  ...         clothing<jackets<blazers\n",
       "4  5da8218fb25b035d1d17b7e8_1.jpg  ...          clothing<pants<wide leg\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's look at the shuffled data which will be used for preprocessing data.\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "oOXdstGGS6u3",
    "outputId": "7a7a5090-26a0-4195-e3d5-972ae0cd7c8a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65714/65714 [20:53<00:00, 52.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Now we are going to read images \n",
    "from tqdm import tqdm  \n",
    "path='/content/drive/My Drive/Colab Notebooks/netaporter_gb_images' ## location of extracted images\n",
    "train_images=[]                   ## creating empty list to save images \n",
    "def create_train():               ## Defining a function to create training data over images.\n",
    "  for img in tqdm(train['image']):       ## Iterating over all the image names in train['image'] to read images\n",
    "    final_path=os.path.join(path,img)    ## Joining path with images to get full path of images \n",
    "    train_img=cv2.imread(final_path)     ## reading images using cv2\n",
    "    img_arr=cv2.resize(train_img,(64,64)) ## We will resize our images to be a (64,64) images.\n",
    "    train_images.append(img_arr)       ## Saving resized image in train_images list. This proces repeated untill we reach last image name.\n",
    "create_train()   ## Now calling our create function to get list\n",
    "print(len(train_images)) ## Printing length of train_images to see number of images in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "colab_type": "code",
    "id": "UbJfKAWCUSEB",
    "outputId": "b12bf11a-cb4f-4a0f-eab3-3bb5d6a808e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['clothing<jackets<casual jackets', 'clothing<tops<t-shirts',\n",
       "       'clothing<beachwear<coverups', 'clothing<jackets<blazers',\n",
       "       'clothing<pants<wide leg', 'clothing<dresses<mini',\n",
       "       'bags<tote bags<tote bags', 'clothing<skirts<midi',\n",
       "       'clothing<tops<tanks and camis', 'clothing<knitwear<medium knit',\n",
       "       'shoes<pumps<mid heel', 'clothing<jumpsuits<full length',\n",
       "       'clothing<tops<blouses', 'clothing<dresses<maxi',\n",
       "       'clothing<dresses<gowns', 'accessories<wallets<cardholders',\n",
       "       'jewelry and watches<fine jewelry<earrings',\n",
       "       'bags<shoulder bags<cross body', 'clothing<dresses<knee length',\n",
       "       'jewelry and watches<fine jewelry<rings', 'beauty<makeup<lipstick',\n",
       "       'clothing<dresses<midi', 'shoes<sneakers<low top',\n",
       "       'bags<shoulder bags<shoulder bags',\n",
       "       'jewelry and watches<fashion jewelry<necklaces',\n",
       "       'clothing<tops<shirts', 'shoes<boots<ankle',\n",
       "       'beauty<makeup<foundation', 'clothing<knitwear<fine knit',\n",
       "       'shoes<pumps<high heel',\n",
       "       'jewelry and watches<fine jewelry<necklaces',\n",
       "       'shoes<sandals<mid heel', 'clothing<beachwear<one-piece',\n",
       "       'shoes<sandals<high heel', 'clothing<coats<long',\n",
       "       'jewelry and watches<fashion jewelry<earrings',\n",
       "       'beauty<makeup<lipgloss', 'clothing<pants<straight leg',\n",
       "       'clothing<tops<sweatshirts', 'lingerie<sleepwear<pajamas',\n",
       "       'jewelry and watches<fashion jewelry<bracelets',\n",
       "       'beauty<skincare<moisturizer', 'bags<tote bags<mini bags',\n",
       "       'shoes<sandals<flat', 'clothing<coats<knee length',\n",
       "       'beauty<skincare<serum',\n",
       "       'jewelry and watches<fine jewelry<bracelets',\n",
       "       'beauty<makeup<eyeshadow'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's look at the unique classes in training data\n",
    "train['classes'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4CwHrpF5bAF2",
    "outputId": "5f09177f-8fad-4668-fbe2-9fb4321de784"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Looking at nummber of classes \n",
    "train['classes'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gpv8RPVsTknM"
   },
   "source": [
    "We have 48 unique classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvh0Y4RYbkaa"
   },
   "outputs": [],
   "source": [
    "## Creating numerical labels of classes in our data\n",
    "train['classes']=train['classes'].astype('category')\n",
    "train['labels']=train['classes'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "Cbf8-jObcd-i",
    "outputId": "6a98ae6f-928b-4410-cee5-b037532b42ce"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>name</th>\n",
       "      <th>classes</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5da81e6d465e00845f490b5b_4.jpg</td>\n",
       "      <td>Acne Studios  Lavinia shearling jacket</td>\n",
       "      <td>clothing&lt;jackets&lt;casual jackets</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5e497e394c033d18b291a5b7_4.jpg</td>\n",
       "      <td>AMBUSH®  Oversized tie-detailed printed cotton...</td>\n",
       "      <td>clothing&lt;tops&lt;t-shirts</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5e4d39024c033d562da0e19c_1.jpg</td>\n",
       "      <td>ViX  Spring Romance shirred floral-print stret...</td>\n",
       "      <td>clothing&lt;beachwear&lt;coverups</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5da81accb25b035d1d17a519_1.jpg</td>\n",
       "      <td>Balmain  Double-breasted wool-twill blazer</td>\n",
       "      <td>clothing&lt;jackets&lt;blazers</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5da8218fb25b035d1d17b7e8_1.jpg</td>\n",
       "      <td>Max Mara  Kenia wool-twill wide-leg pants</td>\n",
       "      <td>clothing&lt;pants&lt;wide leg</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            image  ... labels\n",
       "0  5da81e6d465e00845f490b5b_4.jpg  ...     21\n",
       "1  5e497e394c033d18b291a5b7_4.jpg  ...     31\n",
       "2  5e4d39024c033d562da0e19c_1.jpg  ...     11\n",
       "3  5da81accb25b035d1d17a519_1.jpg  ...     20\n",
       "4  5da8218fb25b035d1d17b7e8_1.jpg  ...     26\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWrcxWKkUAss"
   },
   "source": [
    "We have now numerical labels to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "colab_type": "code",
    "id": "99yTdBzxe9Hg",
    "outputId": "0e096756-bda4-458a-c01f-062ea8e9cf54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe9cee27390>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's have a look at the label distribution\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.countplot(train['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0gQL4twUQBF"
   },
   "source": [
    "As you can see label 18 has been occured most number of times which is around 5000. There are many labels that occured for less than 1000 times. There are seven labels which occured for more than 2000 times. Which means Precision, Recall and F1-score parameters could also be used in this scenario along with accuracy. However I am only using accuracy parameter in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gkm-AWw8eJQA"
   },
   "outputs": [],
   "source": [
    "## Now let's create an array of training labels.\n",
    "train_labels=train['labels']\n",
    "train_labels=np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "li5Np8CKgUTk",
    "outputId": "eef40eb4-5f74-4ff8-c4da-9c71747cae25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65714,)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NfLzc5zyXfbs"
   },
   "source": [
    "However our training_label does not have an appropriate shape (65714,) so we will reshape it in an appropriate shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vATj7xkbnLfL"
   },
   "outputs": [],
   "source": [
    "## Reshaping training_label to get appropriate format\n",
    "train_labels=train_labels.reshape(train_labels.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e15PvF9EnLbY",
    "outputId": "3e792b7d-9497-4933-ffa0-28a98bca63bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65714, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NbAlwi8vYZUP"
   },
   "source": [
    "We have 48 labels. Hence we need to one hot encoding inorder to do classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V5g20bpznLYs"
   },
   "outputs": [],
   "source": [
    "train_labels=np.eye(48)[train_labels.reshape(-1)] ## np.eye() will create an Identity Matrix which will use training labels as index to\n",
    "## get one hot representation. For example if everytime label 0 appears first column will be one else zero. Simillarly for other classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KkqtWmRinLWL",
    "outputId": "89a28a8a-b43c-457f-8229-7b96931f285e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65714, 48)"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUWauj5gciUO"
   },
   "source": [
    "Now we have one hot encoding of labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "_oVtbkNwnu7x",
    "outputId": "c4bf5d1b-2d4e-49f1-b922-04756f26ad7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1jbOjGJCcq_L"
   },
   "source": [
    "We have stored images in a list. But to run algorithms on top of our data we need an array of a shape like (None,height,width,channels). Where None is nothing but representaion of number of images, Height and Width are image size(pixels) and channels is used to represnt a graysacale or RGB images. If it's a grayscale image then it will be 1, if image is RGB then channel will be 3. So we will convert our image data in this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V5HN6Qu9euTl"
   },
   "outputs": [],
   "source": [
    "## Converting our train_images data from list to an array\n",
    "train_images=np.array(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "N1fgNXg8ff4a",
    "outputId": "64fc4112-9eaa-48ab-b9cf-9d3b3722fc54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65714, 64, 64, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ofdOtuDgd898"
   },
   "source": [
    "Now we have an appropriate format of image data to run algorithms. In next step we will divide our data into training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F8t3AyfVBlNe"
   },
   "outputs": [],
   "source": [
    "X_train=train_images[0:60000,:,:,:]\n",
    "Y_train=train_labels[0:60000,:]\n",
    "X_test=train_images[60000:,:]\n",
    "Y_test=train_labels[60000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jC4sAFCLRRJC",
    "outputId": "333aa145-89e9-4009-b453-0d6737be9e6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 64, 64, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aJZyjKmrRUoj",
    "outputId": "fd64b0c5-2624-4ac3-98a4-83d63ee66649"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 48)"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "JK2ZM_CWUAB1",
    "outputId": "fdde3197-476c-4554-d95a-446874462632"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "clothing<tops<t-shirts\n",
      "AMBUSH®  Oversized tie-detailed printed cotton-jersey T-shirt  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO19eZhcV3Xn79TeXb2qW92SWjuWLO82CGMwGGNjx4BZhvA5EJKxJ+ZzmEBCCBmW8CUhyTCBmQyEAQI4bJ4JiW1WG8JmGxsMBNsy3iTZsmRLtvbW0vtS650/uvTOObe7nl6XqquV1Pl9X399q+59991337v1zrnnnN8h5xwMBsN/fMQWewAGg6ExsMVuMDQJbLEbDE0CW+wGQ5PAFrvB0CSwxW4wNAlOabET0TVEtIOIdhHRB+s1KIPBUH9QrXZ2IooDeBrAVQD2AXgIwNucc9vrNzyDwVAvJE7h2IsB7HLOPQsARHQrgDcCqLrYu7uXuBUDA3PWUdiZQisXBxQ2qEUcb31OXYdeZBeuyvcAnt65OyiXi3lVF0/w41kqFoNyoaDbtbQtDcqJRMrrIyXq4nMOCQBiMa6Lx/UgSXxMxOOqTvaZiMdEuazaJfVhGtWm2x9khNuyd+9eHD92bM6Wp7LYBwDsFZ/3AXhJ2AErBgZw29e/BQAg8ie0+pVQTE4ctws7ZlYfEdvWo12tfcjPUuKaj65V7dwLMVdR+5DCo4vpJ/g3rv3PQXl08DlVt2QJL+KhoaNB+cAB3W7T5rcG5a6edaqus2d5UO7p7uJxeNfY0t4RlNuzelmk0nwHers6VN1S8bl7SZbP1ZVT7QY6i6iGavPtS91R7stvXH1V1boF36AjopuIaAsRbRk6fnyhT2cwGKrgVN7s+wGsEp9XVr5TcM7dDOBmADjn3POqbhDIXzH/F0z+wMmqWn75FhthY4y6f1JvKSVsHPWRdLhcLml51hULQXl0bFjVLVvGKl9ra2tQ9kXiYp7fokePHlF1pTK/bamUDsptna2qXX5kJChP59OqrjXOy2T84Kiqa9u4MSgv7ea3d09n9Lmvdt/rLY2dypv9IQAbiGgdEaUAvBXAnafQn8FgWEDU/GZ3zhWJ6N0AfgQgDuDLzrltdRuZwWCoK05FjIdz7vsAvl+nsRgMhgXEKS32+cPVRT+p6cw16KGNRtVxNZhzIPL8OKEFkjY1OcefCUlRoxXuI4d5myeTyai6sbGxoDw+Ns59k9apE5kl3EeL3i0fGeV9gKGjU0E5ldF99Pb2BuWeZXpZ9PS3B+Uzz1+m6s5cx+dra2eTYAfp3fi8C7O9NQbmLmswNAlssRsMTYIGi/Gk7TACSrz3RELprRZuoltYUX2xxH/fAUR+8nxU4GLRnGpqmatSWT8uiQSbzch5Y3QsJpdjLN6+8uXX6nYlNlfFEtocNjQ+weXBwaDc3X+Wardk+Zqg3Lt8uaorFHiMg/tZpM9PazH74MGdYhzrVd22R3lb6jdfq0NAujt4HltSJT6vq/4eXaznyN7sBkOTwBa7wdAksMVuMDQJGm56C8xIYW6jZa8uxr9J5OnzjUQ1PXfhzYbeFzWcrh6uxUlXUp/zJRGVhoKqSzv+fPWrfisoH58aU+2oxC6xA0tfr+oOHH4oKJ9zyeXcd0e/ate6hANc+pf1qbrObjabrVjOrq6tWb0/8IIz+bilndos15JjHX5Fv54DwmRQjot9i5I3vfHTIHTT3uwGQ5PAFrvB0CRorBjvokV2zRIx1TH8++TgmejCRFPl7dU4j7R6iPjxkC58jSfq2dSvfEmLrdNgT7N9z3cH5Y/97c9Uu/zoiqCcyXapun/6DHuk3fdDNl3t2dWi2n3yy3wvDh/WQZNrz3hHUM4JE9qxQwdUuxUX8RizWa0m9PWwV96yJXzuOGmSi/ZW/pwsabPceRv4OhNl/X4s0NzPY8wzRTqceiThqZqW7c1uMDQJbLEbDE2CBu/GM8J2h6OTOMyj/+hDCznf/OmDwurqsYkfVZxLkP5dHy91BuXf/53vqLqJCRZby8LTLNuzSrUrFoTYWtCqwF33MivRa17TE5TPPVuPd3JyOiinklrE7+zjIJNEnPvPtmmV4fCTPMf7n1iq6na2chBOdgnvnJ99vu5jaR/vspfLmgADZZ6rYmxaVSXF7fVVKolaxO5an6tqsDe7wdAksMVuMDQJbLEbDE2ChuvsJ3SNepgVwnT7qMSORNF/7+odVef8SLE6eMbJTmR3Tz+jTU1/8p4Hg3L58BJVN1Zmfbt/NXuPpVp6dLtjTO+8YmWnqnvZK/h8ReF554p6vNkU69GpstajB59nj7e+Vdx/tltPVJrYG87Fdf/lHO85xIn73/6IbrdvF+v28awmqDh4Bvfxxiu85yWuPeqCcXifw25tCK2+bmemN4PBEAW22A2GJsG/b9PbrN8qIdr4XnIygEYRC4Rx1PvuaYsXhBMZgmCi6DgI5J3XaV7Q6SkWnzNZLZ73DawMym0d7AnXntX8bmXBk/70tvtVXSG/OSiX4m1BOZnQj9zWXzwWlC943WWqblQkfjkm+NpXntmr2qHMN218THu/JeNCPBfPRDKhOeEmp0XGlrzO3vLsPm77xVvbVV17hkk1fus3hTcd6essiQcrjhA+ugV8xuzNbjA0CWyxGwxNAlvsBkOToKE6u0Ntprd6mLx0H0IvCutvVvTd/CPnwnOg1cZKIfv0NbxkTJBGXPCNoHx0ZFC1625/QVDuXK710GSa86MNiRxovSu0KyqO8zjO3qAj1sYHzw/K8W7Wo7t7tQkw08nnXrZMu8vmxni+Dz53KCj3jGozXzLD7dq7tNvu5Kgw+4mItUJJk23IumRK95Esct04JlTd1BSb6T7/BSbWfN9NqhnGp3mPJOZliS1D7jMsHMnFSd/sRPRlIhokoq3iuyVEdBcR7az87w7rw2AwLD6iiPFfBXCN990HAdzjnNsA4J7KZ4PBcBrjpGK8c+5nRLTW+/qNAC6vlG8BcB+AD5zKQOoRFeRVVu8/xPSmDwox30UUt8JNjF73VbqcZYwJUWsmCmxqGptkc1VX22rVrm89i5+5KS3StmdZjO9bw9xsSWgsW811/+dvfk/VPX4/280SIsfy/b/Kq3YQ0Wx5ryotpPpMO5NQHDmkUzv3r2YPwPaUno+SEPGnpnkmW1r01TiwuD98VJveSiVuu36j5q4rF/l8Q2Pc/99/6bBq90c3sDkTU3qM4xlWZZLEXokFz0SXFCm1XA3Sfq0bdP3OuYOV8iEA/WGNDQbD4uOUd+PdzKur6iuWiG4ioi1EtGV46Hi1ZgaDYYFR6278YSJa7pw7SETLAQxWa+icuxnAzQBw9jnnuqjecRLVvOsWPo1OWP9hzBkRu5jlvVfLten5/Oif/SIod3ay6J5s93a6p5hnLh7TO+SpVhaZO7tYpB8ZGlXt9m5l77fC2PmqDjkWOUeP8LV85h8eU82yvRxoMz2pr6W7j0X8sRH2whs7rseRm2Q1pNShRd9sJ597SvBO+HTlyQwfl2rVwS3yug/t05lme/t4CbW1sIh//w/00nrvDdx/blqP8fNf5P5v/EO+FxmPnrss1EqqYde+1jf7nQCur5SvB3BHjf0YDIYGIYrp7V8A/BuAM4loHxHdCOBjAK4iop0AXl35bDAYTmNE2Y1/W5WqK+s8FoPBsIBYtKg3H6EedKIcObi/hvPOOneofSOq511YFNOpp2Tyt0B+eS+fL5NhPb3F09ldWZBFekSPHUs5ui03zbp9wjO+Pfb0zUF531MfV3VHhlnf/O537grK1KrTLWfSrKMW8t58CEe2bCePcWpCc8NPjrMyPj2lx9jVy/pxMsWPeyKtz+VEvqZMVuvUhUke43M7J1XdqnXsTzY5xbp+vEX3/8d/ylz3f/dRbbzat4u9D2//KpN93nC9aoayMMXNK2dCBeYbbzA0CWyxGwxNgoZnca3J9KZ6qI6YyPbqS89OijkxFrdmc9BF465TR4QF7oQRbHhX46pcnT/GuOBZi8c0X1pXJ3/OiNRH0xOa7zweZ9G0u0v30ZJlkXn0OIutRw5r8fkPrr88KN//kx2q7pmD7FNx2/e/F5TPWX+GHkea3eamc3qMuQKLrZksi+eZNq12FHLs8ebfi6lJfhCy7WJ+nRbVp6dZ7cjpWBcgxn0WtXMddu/iL3qX8nI6/xXLVbuv/c32oPzhj2j+/cHDbCJtH2LzXdEdqzaMWYiyruzNbjA0CWyxGwxNAlvsBkOT4LRJ2RzK8y7bhXWv2SJ1/6phyG9cTEQWhbULGVOY9qRNjJG6hyt7t6nM+uVNN92p27ZzBJUrsv6XSGkd1TnhltmhiSRLeZ6D4UFWYNtS46rdGy59Q1C++67HVd2hSdbv12Ru4L4L2gU0J4gvhw+NqLqOHo5mK4ukat3dmmzj6CC7m0r9HQASSb6HLW08jwW9PaAQi+n5dmK+fTfV4UEmnli2mveCcsPaBfniN20Myh/4sL7O9AC3TQjL4fERfV+WdA4F5XjMv5+msxsMhgpssRsMTYJFS/80z4O4GFvg3yc1PB39RFX4vss1ptadPRdzexE6p8eRF2L9vv1apE0LxodSmc1a2Q4drZXIsKdWZ3dW1Y0L8oapES7f+9h7Vbv28cuDcqZjpap76Ocs3rocjyNV0mazsuBoP35Eh0DHUzzmcpnd6dJp7SXX08dzUCppMT7bIXj0ReqpdIsWx4tFbjc1oe220ruOYt49E/awZ7ZzXa+2ZmLNGTxmghbxt29hneK48Oy77etTqt273iGX6/zXkb3ZDYYmgS12g6FJsGhU0qHtvDbSM64mNQDzSC8lgl/CdssbSaKRjGnRdPdRFoX7B7Sn1tgQ74K3CK+z0TEtmq5dwXTM5Kkrhw8wf1o2xefe2N+n2rW2DgTlA8e9Hewci/WuzGJqwkv/VC7yTnfME03LBaFOTAlVrqzHKz3+EindvxTdJXs0xXSaqISgj06l9DtwWpBGxGK+NyN/JmLxfOiwnu90C7db8wL9vJSFmlMosAXl2FGtohVjR4JyMiyFVBXYm91gaBLYYjcYmgS22A2GJsHpaXrzwnvKQpfzLR/V4OvRZUnWEGq+q+6vV01Pr8c+wszZxLhElwXSOuqnPvo013lkDb3L2Ovq2BHWlXv61+hzxfncE2NetNkE65C/eprTAbzq/Bepdgcm2TT06K/1o5Qg7iNOQr/0bmBRkMWX8noc05PsXdeaYsLJQsEziU4JE2NSz8fkKF9nV5+IXpvSKZ6KRe4z7ZFXJCdEn+TpyvG5n6Vc3gu7lCmi/SjGVu5/+Kgwt6W96L5p9ihMZDR3fhTYm91gaBLYYjcYmgSnJ3lFqXpeJOlBFxo8E8Ytp9p5Xwg+cb9KSv/1yTrreWopQgVpatJH5Up821xS97//MHuh9XVz8EXGMyel4tzH8wcOqLrSBJuQVnYIjjWPI+6ZrZKvz59vNpsVBXdd2fMGLAs2CF+9krEeilDjmCbRKBb43NPTWhXo7GXzVbvwkmvRDoWYmOBrmfTyUKUyYpnEqnvXTU3zcXkvIKelhdWQ3JSeg/FRkeFVmCY7JrWq8cO72Vz4ltf5wVEz/yiEN9He7AZDk8AWu8HQJLDFbjA0CRaNvKJ2PXfuY04K0WeYGS6qLh5+qqhmOe+3VqSELpf5uC9+RkeDtWdZH/zR9zVfe89SNrGtWfluPlNmnWqXyoj8aJ4V52s/Zhfct/+ntUF57wHtvlnK8+eUZ1KTnyRfO5W9DQjxOdmmo+9iCZ6ffF6ShGrzV06Y3pJpXTc+xKasfC+bJRPtWm+WJBfxnGcaE2mfyyV/yXCdNN/FPPPx+CgTf0znddrn0QNMSpEQexPjS/R8P/W0MD++VqeEjtPMuKqRluqRVgERrSKie4loOxFtI6L3VL5fQkR3EdHOyv/uk/VlMBgWD1HE+CKA9znnzgZwCYB3EdHZAD4I4B7n3AYA91Q+GwyG0xRRcr0dBHCwUh4joicBDAB4I4DLK81uAXAfgA/M0YXfHwAtSgMLJLpLyPPVTC4x/3Zhx5V9m1qcP1OZzV+3f/OrqtlkmT3olq3Xol48zpxxD/z6I0F5+Lj+Xb/s0r8LyrH4L1TdKy+5hfsrXhSUC+MrVDuCTIWkzURxEZUVEybG/LQmZEhm+Dj/mRBBb+hq4flwRd1ucpjHIfnwAcCV2Ma2dyfzsJ9xUZdq19rO98Vz5MP4qIi+m9DRcu0drHpIEbo165GFiPdqflKfoDjMx7W183wUp/V1HjjCJseSR4CRrkQuhq2ieW3QEdFaABcBeABAf+WHAAAOAeivcpjBYDgNEHmxE1EbgG8C+GPn3KisczOvtjlfb0R0ExFtIaItQ8NDczUxGAwNQKTFTkRJzCz0rznnvlX5+jARLa/ULwcwONexzrmbnXObnXObu7tsD89gWCycVGenGSXzSwCedM59QlTdCeB6AB+r/L/jZH1JpppazVpRo9dm6S5VdOywvYMws1y1Y04G1Yc//BJ/kRNXsOGyF6hm48fZNDY+oaWl0RHWS3NTIn/Z+BHVbvog67nbdn1N1aXim4JyfowZZ2LedSYTrDeeMP0wuG1cmMN8wkZJ1umb5crSlCXcQOMJbV7LZlvnPAYAikU2yx3dfzQor9vQo9ol2vjcLa36Wg48z4Ks/0wMHWeTmmStKea1bl8u8+fOrPdcCS59uSfgjioBGp1LmClox259389fc/L3dhQ7+6UAfhfAE0T0aOW7P8PMIr+diG4E8ByA6yL0ZTAYFglRduN/juqbfFfWdzgGg2Gh0HDyCiaiCIlKiyjSh4n+nlELsYjEE1TF027WGCMiPJ2zP35u+8QT7DWXadciZ4nYrNPa26vqlicvCMrZVjbLbf3pT1W7vQe/GJTbsEnVjU8ySUJJzGTaJwKVkXleNFgiLggfYlK09iLbQqIYi9OsauRzLOrGvVRWyRKrE9MlbdqTfXb3sMFo+68PqnZnXcyqUbpFX2drO59v0iPulASlhRJfp/9cJZPcx3c+v0vVdXfwfZIEJuWino8D+w8F5R/d3abqzvr9icpwTsGDzmAw/MeALXaDoUmwaOQVobvgoX5Aord5eK5V28WvNSCnHrxzPtGAE15nv976RFBua9XeWHFx3LhHtCDJIaZEKqSNl75MtfvMh3m7pZjXvG2XbvzHoJxKsIdYuehdsxDPY6Q9uiTxhOQQzLRoXrW88IZLedxsyQR7k42O8s50Z6/2fktlePzFoiaNkCPOtHC7Y0cnVLsj+1llWLFej3FgNc/BwX26/+lRVi/GJ1iFSKY8T74Cn/sVv6W5/nf8UngiCmtFwbuW9iQ/B7njeoyxUmUX38grDAaDLXaDoUlgi91gaBKclrzxUbXhWvXmqF54YWa5euR3m5XTTphNfve6q4PyX33ubtUuJzjZW5LaDCW5OtPCc621rVO1e9/HfhaUf/gPX1F1iRgTYCRKnHu4LaPTMquUayEmH1mXy3k51jLC+82b0gmhp2fkPk5Xh2oXFySNfrSZJKCMCzIMP031czvZ1JnO6JiujqV8XO8yPcipNtad48d5QsoFvbSmJ3mPoG2pzpk3NnhPUO5fx/O99iw9p1dfw7r+hZdoT8FYTH+eC/ZmNxiaBLbYDYYmQWNTNkfkoJslEErxObT/+nLELTShRtn5gTZcThMHOvzlOy9T7f7y0z8MyiVPfI4JHvNiTniTtWmSi84uNl9d+c53qrp//cLN3C7OYvFUQYvI2TR778Xjeq6k6S2VYTPUrBmVwS7e05hICJOa4GQ/flgHgaRa2UTX2aNF/BR4zJJcIt2qyTZWCO63w/u1Wa5Q5LbdS/U9a+kU3HVpVkkKHgFGYZLHsW+vDnD5wdZXB+XWNAcyIe6pkTINWlkTB0r++mqwN7vB0CSwxW4wNAlssRsMTYKGm95OwHeXlSYw3+OPqujzoVFpvr4t+wjT0+UhVVt5x8yDiCM0uk+Z9vjWpOLaJfbj73lVUH7///5XVZeXUXvC/DOp1URQB+vsy1drPfdt770pKN/2yX8JyllvuOky67Iurk0/Dqy/OhG9ldBepADY3RRFL5pNuJwm4rznkPBcUeU8jgkyCQBIiRMmRLRcMqPfc3Hhqtvj6f2Swz8/rceYFtsYLsFzkPQmK5fiZ3VFl977+PF9TCT5ptfxfLhZ72Kex1Lcf8bmjiaVsDe7wdAksMVuMDQJFs2Dbj6ib03itNefi2hukzW1mt7qHS2X9IghijEWTT/x/tequo9+9gdB+eAIjyOX0+ItDXNEVdkVVN2S5UuD8o1/8Y6g/JWP/qNqlymyKkDQZApJwcdGYPE2N6nJJZzwBoyndJTX8PTjQbml/5mg/MKNL1Xtnnni+aDc3XGm7mOMH/FinuetXNKRfjmR8inhkWOs3rAqKNPRtapukngOWtt4DsoxT9xP8H1PtWs1JC9UtoITEXDesyK5530uvyiwN7vB0CSwxW4wNAkWbTc+DL6oKwMkpPQyr11w2S7s3DVw4dUj22sYSp6nXUyIxSVPXPyLd78+KD+0ayQof/n2B1W7qTKLi4kJ3b/cuO/sZD663/sff6Da/eL2XwblwUeOqbr8BLuQ5QqC1CGmLQstKQ78WPOiLaruJS/jgJGW1oGgfOZZZ6h2hcs5cKe9XXsKOvHwtArijERcz1uL2FZPemL8Jz/7+aD8kY/8laobH2ZeuO079gTlr39Dp9Ta8gvmvDvn/LerukyMiUV27uF7cd46rb6VRLqwWsKw7M1uMDQJbLEbDE0CW+wGQ5OAak6BXAPO3HSW+8KXvjJnXRgxRDUySp94Iiq5RFiKp7A+wogqq/XhQ/bhp1OiKkQaoePwTTBlNilRnE1qJVqqmr3vf94WlOMl7dEVS/KeQHs7e5NlBNEEAKS6WD9uLWkCxFs+zDrrktb1XNEyptq96rd/HZRftvkiVdffx1F1qZTwKEzqraYYyfnQc5UQxBZST894XngU53b+M/H8gb1B+eDBw6ru8ktfGZTLMkWVN47JIkfSffeb96q697+fM6d98M9vD8pnXqq57V+9SRB9lLW51LmZ+/6a11yNxx57bM4H8KRvdiLKENGDRPQYEW0jor+qfL+OiB4gol1EdBsRzXKENBgMpw+iiPE5AFc45y4AcCGAa4joEgAfB/BJ59wZAIYA3LhwwzQYDKeKKLneHIAT7lfJyp8DcAWA3658fwuAjwD4XP2HWJ3YYj4qSL0546L2F+pNN/sEcx7nX6eaDy9qSKZkkrwWf/7uu1S7Zx5hEblzo/Zq6zubTTxDggeupaBFxy4hSo55nO83fe6aoLz1JzuC8qFBnTF246Y3BeUzNqxRdYUye9S5Ip8rndTebyTE+lkqoFBzEjLYys8mqxKG6WtZt3JFUP7Od3Sy4ldfdnlQTsVZuC15lHDJLHPe/e4Nb1Z1b3jzVUH5qs23BOUlP9X88tuuY3Xov16jU4Kh4mUZtiKi5mePVzK4DgK4C8AzAIadcyfuxj4AA9WONxgMi49Ii905V3LOXQhgJYCLAS8TYAiI6CYi2kJEW0aGh09+gMFgWBDMy/TmnBsGcC+AlwLoIg66Xglgf5VjbnbObXbObZa8ZwaDobE4qc5OREsBFJxzw0TUAuAqzGzO3QvgLQBuBXA9gDuq93JyRCV8kCrqQpBFqqNCTID1MFn6I6zWY+i1lL2jRIRcybGp6d9+uUc1S8TYFFfark1qO+5nF9Cr3sm6fVnzMGKwwF9kWzSB4+5dTwflvgvY5XZT//9S7fY8d19QLuYeUnXPHTkSlF9y/jlBeeML1ql2WckgUdRMjzERISjNcD60ydUn4pDus7pOavpFcVzZj1Qs8xwfP7xb1S3vZ9383JdwdN+2g9r0duAI72nc/bTu/8pNJ/Yxqj8rUXzjlwO4hYjimJEEbnfOfY+ItgO4lYj+O4BHAHwpQl8Gg2GREGU3/nEAF83x/bOY0d8NBsO/A5w25BVRjpmFeZi1op5NkVeEjKMeqaDK3ud6+C5XNVPmtZdcrsgmtUJCm9SSjkXyB/+fIF1YqgkwzrmKReSiF0VGIg/V4H5WC3L6VNi44YqgnG7R5BUDrSzG7j58ICjHknp7KD/GXnnrN+iIuFSS+2wV4n5bVqd/SgjTYdl5aZ/FRG7cuFHVTU+z2TIt1Qnv7h7c92hQ7upfpeo+cTMTjlx6PZOFvMlLTd2ZEc9ZXC/dJ5+fUV+m88ZBZzA0PWyxGwxNgoaL8VFE3qicbjKYHwDigsiBQmiaFxNKFfDrovYhKbTDgoZEVVun3i2fGBNpkYp6m71zGe/UF3NMNiFFUQBIPfxcUD7/JZoLb7jA96Ktk3f0D+5+RrVLizF29+pMs/EUjyPexiQXu0f0NQ+N8hh7dZJYfOufmDfvr//0PUH5wUcfU+16l/MYz914lqorTrLu0d+/TNXFRYCOpNN+drfecV/RyxlwP/3Vh1XdhguZGvzAYSYcWbVCq0ZOPDGDx7U+dMGymacnJCmxvdkNhmaBLXaDoUlgi91gaBIsmunNR1TzlUz5FPfMPWFpn6udN4y8IirC9hjmwykfGRHNfvLc3QPaS44Oi9RNRd3HjgPfFp+ERxppk9S1r+NU0q3dg6puZJJ1218/uS8opzKa5OLoUfaSK5Q0GeXSvm5Rx/sKJc87bf0Ax2Dd+vW7Vd2553Hk9Y8f4uOuufgc1W5U2AS/+6/fVXWr13H/m87ZoOoyMY7AO3yMiS12PPWUavf5B28Nyq948W+qute/mElAnj/O8/PssH52jo2IDYlJTfC5eslMFFwqZEXbm91gaBLYYjcYmgSnJW+8j2qea/UQkefTRy3BNWGc8rN6qzF4pxrkuV/80m5V9+nPfjYox5zvy8eIkzDReZ5ly9ew+Dk+ps1355/Fj9aRY0NBee8RbTIaOSKIM0qaRKOc57YrVrBZbmCFNn9li2wC/NsPXanq7vzuj4Pyw//Gc3D5S16t2qUTrE5ccZnu42e/Ys64/cIbEAAuvpDF7vYsp386Z5M23110EXucDx/TPHYJ8HWv6eJ7tqpLz/ePHuDj3niJ9sLLFT22jDlgb3aDoUlgi91gaBLYYjcYmgSLprPXmwDS/+xiIdrSQtkAABlDSURBVLpyjSmVayGc9KH2HGaf4KTH1IobbtKpjL/waf6dL3kRWokY64oxxxFlBWidOkFsvlsxoHOslaf5Wl62mXXZe+7XEWtHhzjnXFdqpaq76mImvejJcsSd8GydGWOSue1zE3rv4PWvYf37db/BY4r7TByCLNJ5nOwvPp8jub95xzdV3aUv4jxt8vmIedz2rsRzms3qaLaJaTZvtrTwHkkCmljzyhcxycV0QevzUd7a9mY3GJoEttgNhibBoonx9TCbzeeYam3nM456iPGLhbaM/l3f9uxngvLUhPZqu/iitwblfFFEEkKLjq1tLPq2pnQfSPNcdTj23rvuNVrcL0iPSG/MKccEG/k8z/fBY1qd6O3m1M6+KhQT5PmZVhafix6xe0yoMqmUlw5LqIRvfrPmfFf9lLmcy+sxTo3z55UDWl3JZFgdIjELpZJWr071+bM3u8HQJLDFbjA0CRouxp8QTHxyiaiea1Hpo8nzT4vphlXPGzWbbL135k927lOGF8TiynzrMy16Z/rxp5nwoVBgEfPdf6RpoFuSLJ7HyMsqKgKMJnO821wu6nGk0tx/3hOtpwWpRhrcLlnQ4m1LisX9GDS3nEz/VJ5gYoj2dk2UIR+rkRGdzKQkKKLTHh11Ic/BO4ePczDQvv17VbuLL7okKMdjOgdqXvSREepWMu21K2jVQI3xxAWEOFfam91gaBLYYjcYmgS22A2GJsGimd580giJsEixqHCe8iJozEHC3OOPoxxSVw/zXSOhI+z0tThhagqb70Sc9e3rfucC1S6VYl3TlTWhZULoymlBWBH3dN6YJAn1xhGPcx9TU6yv5vKa5GJ0ir3wxkUZAPp62N3OCXbLYlH3kUiwfuzf9yd3PBmU167W0WYTk+zZ9/xejr7r7tIplYVVDvGQV2xZNCyXff76U0s5FvnNXknb/AgRfa/yeR0RPUBEu4joNiJKnawPg8GweJiPGP8eAE+Kzx8H8Enn3BkAhgDcOOdRBoPhtEAkMZ6IVgJ4HYCPAvgTmpG3rgDw25UmtwD4CIDPRT3xfHjbqrULq5tl1kJEs1ZIH/VO/zQr8KXu5rbayDDKJN8BLFb2LdWiqUx9VM6PqrqEMButXcWib0dHm2onz53LadL3ojDTZVpYFUh6qkAmIznwtfmuJMx54xMi5VVRn6u3hwkx0mmtkizrY/76fQd1IM/QCBNztLezh97GDZtUu9wEny/j8fClUhzwEhdpncL4KKqtEV99lYj6Zv97AO8Hm8l7AAw7pi7ZB2BgrgMNBsPpgZMudiK6FsCgc+7hk7WtcvxNRLSFiLYMDw+d/ACDwbAgiCLGXwrgDUT0WgAZAB0APgWgi4gSlbf7SgD75zrYOXczgJsB4MxNZ53adqLBYKgZUfKzfwjAhwCAiC4H8KfOubcT0dcBvAXArQCuB3DHSfuCY10joj7sfyY39/cAEJe6ZkjO5nDdfu4xzdV2vu1mtfVNe1WOqduegO60alWszMcVxaikbglA2ZOmStpMVBImzJ3PPhuUW9O6j/Vr1vK5Ctrl9tgx5kafVgQPWucdFteZSmqjUEzYuRJC1+/p0fsPxQJfy9PPblV1R4aO8gen71l7KxNndLRx2RX03PcIE2DC23OQ+woFMQfJpL5O51jvb3TU2wcws1m3CzM6/JdOoS+DwbDAmJdTjXPuPgD3VcrPArg4rL3BYDh9cFryxksvNsBL8ySIBMj3fhOye6yqUFwb73q9oCPzoqEeBBshWs2s/uMivVISzAOX9zzXZP+tnmgtNAFMjHMU2cSUFve3Prk9KHd16Ei0gRUrgnJ3N3O++8+HmlOPe7AsCCCkuFwqa7tWS5aXwlM7n1R1XV1seuto11F1UqVIChViclJHqKVSbM7zxy8hn/WSFwV4qs+j+cYbDE0CW+wGQ5PgtBTj4zGfjWz+iEoMEZWg4mT9LxZqDcJRtNu+B53Ycf6/X+GsqJsv15lgc9McdFIq6p30bJbbrl+7PihPTmvxtqONPer8+07CujIkfDQScf3Yyp36gkexXBJWAilyS687AEhluI+de55RdWtXivnxHolkQpBqtInMuD4XnlAvfPFcBr/EYjyuYjF6IEyU+25vdoOhSWCL3WBoEthiNxiaBKelzu5H7sSE7lZvYsr56EFOfKyVcLKWlM310Mtnnypk36LE8713D5M+vshpPbco9GO/j8kJ1oFd6XhQzmZ1H9Kc157VZq1EIi7aCbMZaZ23o4M919Ief70kx3AiNbVv/vpvH/iToLxibb+qG53i8bdO6f67SmwuLAiu+LZWvb8RhmRybioIB9/EKOq8WxvFnGxvdoOhSWCL3WBoEjRcjD8hbszy6FpIznRUF90bbUJT56sx1VStddXa+SgK3rkDg8x/PjmtRfDOLk7lNDnlZUUVYnIixe+UWKI6F17B41wrCDKLtiyfK5P2RXUR7BLXgTbSFFco8nU99fRTuh3xOF7+8leouu3b2KOuVNakF8PD7B04sJyzrCaS+jrHhBdhzDMxJuI8r5LcQ44XACC4/i39k8FgqApb7AZDk8AWu8HQJFg005sf9yM1kKi/QDWbvCKa3madTw46Vpv5q957BFGJO+dz3rLIq3b1W84LyrG4NnkdOc7kEuNjmnAyJs6XGue6Yk5Hzk1OssutdHsFgP5+NoFtfuHmoJxO62seGmLTWFdXt6qbyrMbb7aNdf1bbr1FtVs5wISTDzz0kKpLixTOwyPjqi6V4b2Eu+79SVC+9tprVbttT/EewapVmnt+cvJQUF4bWxuU4wnP1FmqHi1npjeDwRDAFrvB0CQ4PT3oQkRTyUGnGBIArQtEFVtrFKvrQoBRI2+8PLefqqgW05vfx9gom4bu+mc2Nd3wXv245AQRRXvWI57oZzPU0qXMv5byTG9+ZJeC4I8rCXH86NGjqllGiNnScw8AMm1c98CDLJ63tOnxxuLsxSbNfACQzbBnX5eX6nlFH4v/5248JyjnvXFsWP+CoOxHva3o5z7K4pmmcnQV0KLeDAZDAFvsBkOT4LQU4xcC1cTuqMEoC4FZI6pBNQjjY4sq0vt9fPZjvKvc0s672ym9OYyeJKc7inn2lcEh3qlPCg86XzSVnGvJpPZ+W9LN4j8Eh5sfKJUWlMuS6w3Qc/DDH9zD7Vp00A2JcUxPaYtBa5qDWhIJPcY48XHpOJ875vR8HxV01CsGVqg6V66iUjnfOzLCbnzII2RvdoOhSWCL3WBoEthiNxiaBI3V2V2I7hxqohJF8fMUpl2XfQKMKq3n5UFXB31+oSPuqvUfptv7it7IOOvpU8KrLeZ0uuVCiXXb8Qkd9Sb17507Of1TT0+Xanfe2cJcVdC68pFjh4Nyr0ifVCrq8boEf/a57fPCtFcQXo+tXgomSSDhPxNTUzwHPilmmzA5yqpcXkfHTea4j6J3L1rEPoMipvS47efxqM6JqPnZ9wAYw0yy7qJzbjMRLQFwG4C1APYAuM45Z2laDYbTFPMR41/lnLvQOXfCSfmDAO5xzm0AcE/ls8FgOE1xKmL8GwFcXinfgpkccB+IevC8RFglv0Tkj/PEdpV8sw78cbVCnc/rz1VrN49xRDW9Kd54z6IzlefglILgfksmte1tYpLNa+mMNkm1Cq+z3iWcQspPCbB79+6gvHTpUlW3YhmbqMYnOAAlk9b8btLrLO71nxW8dmXHIr1zHkc95GctPqeEStLXu0zVtWfZ7EeCM2/4mPbymxBqju+xKFWP1paQNFHu5M+qb5aUiPpmdwB+TEQPE9FNle/6nXMHK+VDAPrnPtRgMJwOiPpmf7lzbj8R9QG4i4gUp49zzhHRnD8plR+HmwCgr99+DwyGxUKkN7tzbn/l/yCAb2MmVfNhIloOAJX/g1WOvdk5t9k5t7mzs2uuJgaDoQE46ZudiLIAYs65sUr5agB/DeBOANcD+Fjl/x2RzljRO8Ii22YRW+j8wvz9fNIya4V4zv5Ohqr68OyG0cbl87VXaxdxHOHtdF1M6KylstaBSxOs28ZaONosQfrdUBYDTrjq0WwlYeZqSWu9P50VudI8d9njgpSiLcvc8DGPOEROo1/3o7vZ9TeV5Gsuek9ZTpj9lnR1qLoNa88MygnPZHdwkM2D3SLltE/S0dXOfea8dM4y75zcf/D3Uk7VUhtFjO8H8O3Kg5MA8M/OuR8S0UMAbieiGwE8B+C6UxuKwWBYSJx0sTvnngVwwRzfHwNw5UIMymAw1B+nTdSbFm/9OlFVI9d61XahfnjR+p/FgR+5x4j918Hk5w+SRAqlb9/6qKqbyo8F5bRjET9f1OKnTFcMj4RCjr9QYFUgFtMqgzRD+fdMms1kH+m0jmyTom+xqM1mBw6xmJ0SfPOU0udqz7J34MbV61VdX19fUM7ltGecFOvlferu1lx4UkXxVQFpYpNRgPm8pWw2GAw1wBa7wdAksMVuMDQJFi3XW6jZzCeSjM2tv4bliwszvUUmgVwAMsqoun6thJZVj/NdLePc7uc/3a3rSqw3ZjrZdbZcLqhm0jSUSulHSZIqhs2iNDv5kNciXUr9a5bmNu32CoyM8P7D9DSPX5rCAGDdwEBQXrKkV9VJbnvfPDgyMhKUi9M8xrZ2HSHY0sL7Bf49knPlhP5e75wD9mY3GJoEttgNhiZBw8V4OpGyOURKjfviimwrxfhZJrrqXOiKY1460PmSUY3edbqLqGY/ry5iH9X6849TdTHt0ZUrsrjrEpoLPZnlCK3zXtgTlOMJLSInRR/5ohbxsyJ6q1WIsL7ZSd4n/55JLzwl7ofclkkvhVRPL4vkk47n4IxVa1S7ARFhV5jW5rVDR9kTfOWKlapuzRruZ//+fUG5hOpkIfG4n7I5JepkWmbdR9QoxmqwN7vB0CSwxW4wNAkWzYMuTOwozwoQqbJDHiLp+hx0YaQR1VArp3ytQSy1iO5h4pyqK6VUu3S78E7La/64uDjs3JfyrvX09JhqJ7OuJr3d+Gwre8rJ1E0j+RHVTu5ELxEkFwCQErxwpRJfV9mbt6RgxNj+1A5Vt3/wQFCOE6sFz+zZqdrlc5xp1g/qWbOORfWxMT0H0hIwMMAivp+RNowPMCmIPyTvXNhzVItXpb3ZDYYmgS12g6FJYIvdYGgSLIIH3cz/sF+ZWR5Goiw1VI9bMHqkWEhVmIdbrUSVuotTM5/4x0XW3Uib3j71t78KytMTHplCO6csTsZYzz1y+KBq19fPZi0/GuzQISZcPPecs4NyzGNkmBZ86seOHVN1Un9dt4ZTHvuRbSRYNFat1ia1lic4ou/o0f1BubtXmxvHxDiueMUrVd2xwxw55/F3YGyCdf0pQSrZ71GwjUwxYWZbVueZy5cF4aSXq66esDe7wdAksMVuMDQJFsH0NrcHXSxMfI7ac1STVER6icjeabUiIgddZPOaBz1GrfTs2ca/8+1ZTQQ6mj8iBsVc7i1tWvycFsEpxZIWrTMpPt/OXWwO8wNQpLltSU9105sMhIl5HmhOvLN++vOfqTqZOjmT5vF3tGoxvjjFqsx3v/99VXfG+nVBeUoExQAAxHUvF154I5PanHlcqCgZQaIBAOedc35Qnpzg/n1u+0iwlM0Gg8EWu8HQJLDFbjA0CRqcstlVJa8II6Wo2p33Oaoraqirroycq9E0Fhl+Lq+I5rxaXG5jMX2rc45NZRM5rV+mW1mnLIn8aEMjnqurILPwSR2Gj7OO2ioi1tqz7apdYQcnF/LdSLu7eS9h/foXoBr6ulhXLpZ0H3ExrA5BKDE+OqrarV21Oih3erzxj2/dyu1Wr1Z1vUuZWDIm3p0tce2evG4N6/1+rjq5r1Aui/fvrAQKRjhpMBgiwBa7wdAkaKgY7wC4igeVK3tmBRnMFptFcs59CLFVG3s87zpfrJFpdSSnnacMxMRnX2qvJe1SPTCfc2l1iOsevF97v41NDAVlaZICAAgPtcI4tztv01mq2ZEjR1ANy3rYg6y7i8Xx48f1Mcv7uJ3vdSbJLFIiXTSRfnbGxlid2LBOc75veWIL95Hmx72Y0HO6/xDPz9i4FvFf9uKLg/Jze59Xdc8f5Ki6s87YGJSnC9pjsSBEcp93Tz1Xin/Ru+/lkHdzpY8wRTPSm52IuojoG0T0FBE9SUQvJaIlRHQXEe2s/O8+eU8Gg2GxEFWM/xSAHzrnNmEmFdSTAD4I4B7n3AYA91Q+GwyG0xRRsrh2ArgMwA0A4JzLA8gT0RsBXF5pdguA+wB84OSnnDuLq4TzyANiclcy5OcpTMSXh8lNzXAxuHr/CnXata/mGRc6xtBsuFx3+9d0iqd0nEVr583W77yDReHR4vagfPio3nGXnmXDQ8OqTu7cDx/nbKwbNmxQ7fJTHIAyNaUDcjrapZcbX9fkpEcMIR7jXs8LT1oMpqfZsuBrWpIbb9Lj03t86xNB+cyNG1VdqcB9as487SU3NcXWD9/q4FTm1ur3PQz14qBbB+AIgK8Q0SNE9MVK6uZ+59wJRecQZrK9GgyG0xRRFnsCwAsBfM45dxGACXgiu5v5WZnzp4WIbiKiLUS0ZdSz0xoMhsYhymLfB2Cfc+6ByudvYGbxHyai5QBQ+T8418HOuZudc5udc5s7OjvnamIwGBqAKPnZDxHRXiI60zm3AzM52bdX/q4H8LHK/zuinLAUeNBpJCVneJipSVTNRzOWWqk03PiefFT1Q3W9iGbb6KqOoxaT3XyIKV1J8sFPi++9iLIU65BTOa2jpjr4d5vKIo2Rd95t21ifP2vTJlXXKbzVjh1jIouR40OqXUcHe6v5XnjaHsvX5TseOkGI8auHHlB1chwtghiiVNLpkPPis/Mi+OJiXM89r01vy4S5sCB09ocf2aLanXv2BTz+kr6f+aLvKlcZR017P9WPiWpn/0MAXyOiFIBnAfwXzEgFtxPRjQCeA3BdDSMzGAwNQqTF7px7FMDmOaqurO9wDAbDQqHx5BUn5PBZDm4y66feSqjmuVYzyYUqe2aQkG2MauYwX9yq1buuNrHNtyGxeD5yjMXWYvmwalYssHibTOugjfZWFluPHeMgmQmPN16al57fu1fVtbTyudOi/9ZWHQQieeN9k5Q0ZU3nRHbTEE+y6YLmwktm4qLM42iJVfdiKxW0WgOhYia99FXynu0/yF54LR7P3ITgp+vt6UEU1PQ8GHmFwWCwxW4wNAlssRsMTYJF4I2P4HLqBelLvVqayuZDUFFrCuQodfPhf68HwYY+xhujcLfsWcK65yc/f7VqJ7Xj3bv3qLrcBHOcTxbYfFfKa516dJR1+FRK6/3Sktom9PT0Km1ea8vy3oGvs6vxlqLdl7xnUivluE7yzS/r1aZIuT/gp1SWpre0d53SUWwiz+6+q5YPqHbSHdfPA5dtYfNjaLqDUyRPsTe7wdAksMVuMDQJqO68amEnIzqCGQecXgBHT9J8oXE6jAGwcfiwcWjMdxxrnHNL56po6GIPTkq0xTk3l5NOU43BxmHjaOQ4TIw3GJoEttgNhibBYi32mxfpvBKnwxgAG4cPG4dG3caxKDq7wWBoPEyMNxiaBA1d7ER0DRHtIKJdRNQwNloi+jIRDRLRVvFdw6mwiWgVEd1LRNuJaBsRvWcxxkJEGSJ6kIgeq4zjryrfryOiByr357YKf8GCg4jiFX7D7y3WOIhoDxE9QUSPEtGWyneL8YwsGG17wxY7zTD7fxbAawCcDeBtRHR2g07/VQDXeN8tBhV2EcD7nHNnA7gEwLsqc9DoseQAXOGcuwDAhQCuIaJLAHwcwCedc2cAGAJw4wKP4wTegxl68hNYrHG8yjl3oTB1LcYzsnC07a6SbHGh/wC8FMCPxOcPAfhQA8+/FsBW8XkHgOWV8nIAOxo1FjGGOwBctZhjAdAK4NcAXoIZ543EXPdrAc+/svIAXwHge5gJ0F+McewB0Ot919D7AqATwG5U9tLqPY5GivEDACTDwb7Kd4uFRaXCJqK1AC4C8MBijKUiOj+KGaLQuwA8A2DYuSBta6Puz98DeD84NqdnkcbhAPyYiB4mopsq3zX6viwobbtt0CGcCnshQERtAL4J4I+dcyqxWKPG4pwrOecuxMyb9WIAm05ySN1BRNcCGHTOPdzoc8+BlzvnXogZNfNdRHSZrGzQfTkl2vaToZGLfT+AVeLzysp3i4VIVNj1BhElMbPQv+ac+9ZijgUAnHPDAO7FjLjcRUQnYjEbcX8uBfAGItoD4FbMiPKfWoRxwDm3v/J/EMC3MfMD2Oj7ckq07SdDIxf7QwA2VHZaUwDeCuDOBp7fx52YocAG5kGFfSqgmYD1LwF40jn3icUaCxEtJaKuSrkFM/sGT2Jm0b+lUeNwzn3IObfSObcWM8/DT5xzb2/0OIgoS0TtJ8oArgawFQ2+L865QwD2EtGZla9O0LbXZxwLvfHhbTS8FsDTmNEPP9zA8/4LgIMACpj59bwRM7rhPQB2ArgbwJIGjOPlmBHBHgfwaOXvtY0eC4DzATxSGcdWAH9R+X49gAcB7ALwdQDpBt6jywF8bzHGUTnfY5W/bSeezUV6Ri4EsKVyb74DoLte4zAPOoOhSWAbdAZDk8AWu8HQJLDFbjA0CWyxGwxNAlvsBkOTwBa7wdAksMVuMDQJbLEbDE2C/w9b99fEr0erUAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's look at one image with it's class and label\n",
    "plt.imshow(train_images[1])\n",
    "print(train_labels[1])\n",
    "print(train['classes'].iloc[1])\n",
    "print(train['name'].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "49-mQtdThYi2"
   },
   "source": [
    "Saving preprocessesd image data for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "POHgj5inXGRZ"
   },
   "outputs": [],
   "source": [
    "## Saving Data for future use\n",
    "#save('/content/drive/My Drive/train.npy',X_train)\n",
    "#save('/content/drive/My Drive/test.npy',X_test)\n",
    "#save('/content/drive/My Drive/label_train.npy',Y_train)\n",
    "#save('/content/drive/My Drive/label_test.npy',Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGFg3UiVepqK"
   },
   "source": [
    "You can load your data by using below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "imcoXkUr02-3"
   },
   "outputs": [],
   "source": [
    "X_train=load('/content/drive/My Drive/data ecommerce /train.npy')\n",
    "X_test=load('/content/drive/My Drive/data ecommerce /test.npy')\n",
    "Y_train=load('/content/drive/My Drive/data ecommerce /label_train.npy')\n",
    "Y_test=load('/content/drive/My Drive/data ecommerce /label_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QZ61Kg2leyuf"
   },
   "source": [
    "Normalizing data by dividing 255. Reason for dividing 255 is that pixel value ranges from 0 to 255. It will give us values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D3Ht-OgIXGUr"
   },
   "outputs": [],
   "source": [
    "X_train=X_train/255 ## Normalizing train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AYTngt5tvsrB"
   },
   "outputs": [],
   "source": [
    "X_test=X_test/255   ## Normalizing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "id": "GLGzBBvNycgE",
    "outputId": "8881b70a-2081-4133-e009-6c37fb61cdef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[10])\n",
    "print(np.argmax(Y_train[10]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4nBpwajM2J1t"
   },
   "source": [
    "# **Text PreProcessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9tb_1PI2XVO"
   },
   "outputs": [],
   "source": [
    "## Extracting product names from our dataframe\n",
    "names=np.array(train['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "kQ0ZH7jQ2XRb",
    "outputId": "b403e30b-5310-4335-a2bb-94bbc9bc2d02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Acne Studios  Lavinia shearling jacket  ',\n",
       "       'AMBUSH®  Oversized tie-detailed printed cotton-jersey T-shirt  ',\n",
       "       'ViX  Spring Romance shirred floral-print stretch-crepe jumpsuit  ',\n",
       "       ...,\n",
       "       'Balenciaga  Track 2 logo-detailed metallic mesh and rubber sneakers   ',\n",
       "       'Alaïa  Bombe studded suede sandals  ',\n",
       "       'Alaïa  Marie-Jene mini laser-cut leather bucket bag  '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nTDHWBDPfmPh"
   },
   "source": [
    "We will define a clean function whih will remove puntuation marks, numbers and Extra spaces from end of the sentence. It will also replace upper case letters into lowe case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qoGygyBs2XOd"
   },
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "  ## In this step we are converting charecters into lower case\n",
    "  for i in range(data.shape[0]):\n",
    "    data[i]=data[i].lower()\n",
    "  ## In this step we are removing numbers and puntuation marks from text\n",
    "  for i in range(data.shape[0]):\n",
    "    data[i]=re.sub('[^A-Za-z\" \"]+', '', data[i])\n",
    "  ## Here, we are removing extra space from the end of the sentence\n",
    "  for i in range(data.shape[0]):\n",
    "    data[i]=data[i].rstrip()\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UgYphx1g2XL0"
   },
   "outputs": [],
   "source": [
    "## Applying clean_text function to get clean text data\n",
    "names=clean_text(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "0iOmlPVM2XJc",
    "outputId": "fdce65d6-aab5-46f7-8a4b-76b7a9db779d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['acne studios  lavinia shearling jacket',\n",
       "       'ambush  oversized tiedetailed printed cottonjersey tshirt',\n",
       "       'vix  spring romance shirred floralprint stretchcrepe jumpsuit',\n",
       "       ...,\n",
       "       'balenciaga  track  logodetailed metallic mesh and rubber sneakers',\n",
       "       'alaa  bombe studded suede sandals',\n",
       "       'alaa  mariejene mini lasercut leather bucket bag'], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w5BJYO8R4CHd"
   },
   "source": [
    " **Compare the data before cleaning and after cleaning bold text**\n",
    " \n",
    " **Before Cleaning :** \n",
    "'Balenciaga  Track 2 logo-detailed metallic mesh and rubber sneakers   '\n",
    "\n",
    " **After Cleaning :**  \n",
    "'balenciaga  track  logodetailed metallic mesh and rubber sneakers'\n",
    "\n",
    "You can see that we have changed uppercase letters and removed numbers, puntuation marks and extra spaces from end of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ALN6L8ItF6t4"
   },
   "outputs": [],
   "source": [
    "# The maximum number of words to be used.\n",
    "MAX_NB_WORDS = 5000\n",
    "# Max number of words in each name.\n",
    "MAX_SEQUENCE_LENGTH = 15\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Rf2J1mIQ5o2u",
    "outputId": "c3e7373c-ec4f-41f5-a0ca-82084dc3a2ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7324 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Generating Word tokens\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(names)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLvCUK2VgIMz"
   },
   "source": [
    "We have 7324 unique tokens which we will use to train our text model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Y16qP4TY5ozF",
    "outputId": "06b1efe9-5dbd-4c9b-9769-54c426c67ce8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dress': 1,\n",
       " 'leather': 2,\n",
       " 'and': 3,\n",
       " 'midi': 4,\n",
       " 'gold': 5,\n",
       " 'karat': 6,\n",
       " 'mini': 7,\n",
       " 'printed': 8,\n",
       " 'crepe': 9,\n",
       " 'floralprint': 10,\n",
       " 'sandals': 11,\n",
       " 'pants': 12,\n",
       " 'bag': 13,\n",
       " 'suede': 14,\n",
       " 'tote': 15,\n",
       " 'earrings': 16,\n",
       " 'the': 17,\n",
       " 'diamond': 18,\n",
       " 'belted': 19,\n",
       " 'shoulder': 20,\n",
       " 'sweater': 21,\n",
       " 'wool': 22,\n",
       " 'boots': 23,\n",
       " 'maxi': 24,\n",
       " 'de': 25,\n",
       " 'ruffled': 26,\n",
       " 'top': 27,\n",
       " 'satin': 28,\n",
       " 'gown': 29,\n",
       " 'gucci': 30,\n",
       " 'skirt': 31,\n",
       " 'ankle': 32,\n",
       " 'sneakers': 33,\n",
       " 'blazer': 34,\n",
       " 'metallic': 35,\n",
       " 'striped': 36,\n",
       " 'blouse': 37,\n",
       " 'coat': 38,\n",
       " 'cottonblend': 39,\n",
       " 'shirt': 40,\n",
       " 'silk': 41,\n",
       " 'embellished': 42,\n",
       " 'pumps': 43,\n",
       " 'oversized': 44,\n",
       " 'woolblend': 45,\n",
       " 'chlo': 46,\n",
       " 'chine': 47,\n",
       " 'beauty': 48,\n",
       " 'lip': 49,\n",
       " 'texturedleather': 50,\n",
       " 'prada': 51,\n",
       " 'small': 52,\n",
       " 'cottonjersey': 53,\n",
       " 'valentino': 54,\n",
       " 'silksatin': 55,\n",
       " 'wideleg': 56,\n",
       " 'necklace': 57,\n",
       " 'asymmetric': 58,\n",
       " 'cotton': 59,\n",
       " 'silkblend': 60,\n",
       " 'pleated': 61,\n",
       " 'draped': 62,\n",
       " 'ml': 63,\n",
       " 'leathertrimmed': 64,\n",
       " 'rose': 65,\n",
       " 'turtleneck': 66,\n",
       " 'embroidered': 67,\n",
       " 'miu': 68,\n",
       " 'jacket': 69,\n",
       " 'crystalembellished': 70,\n",
       " 'checked': 71,\n",
       " 'jumpsuit': 72,\n",
       " 'wrap': 73,\n",
       " 'ribbed': 74,\n",
       " 'faux': 75,\n",
       " 'balenciaga': 76,\n",
       " 'pearl': 77,\n",
       " 'by': 78,\n",
       " 'cottonpoplin': 79,\n",
       " 'velvet': 80,\n",
       " 'tulle': 81,\n",
       " 'saint': 82,\n",
       " 'cropped': 83,\n",
       " 'laurent': 84,\n",
       " 'ruched': 85,\n",
       " 'cashmere': 86,\n",
       " 'straightleg': 87,\n",
       " 'leopardprint': 88,\n",
       " 'mesh': 89,\n",
       " 'christian': 90,\n",
       " 'louboutin': 91,\n",
       " 'twill': 92,\n",
       " 'net': 93,\n",
       " 'sustain': 94,\n",
       " 'woven': 95,\n",
       " 'doublebreasted': 96,\n",
       " 'sequined': 97,\n",
       " 'linen': 98,\n",
       " 'twotone': 99,\n",
       " 'goldplated': 100,\n",
       " 'tshirt': 101,\n",
       " 'ring': 102,\n",
       " 'rossi': 103,\n",
       " 'foundation': 104,\n",
       " 'gianvito': 105,\n",
       " 'chiffon': 106,\n",
       " 'shell': 107,\n",
       " 'patentleather': 108,\n",
       " 'pajama': 109,\n",
       " 'cutout': 110,\n",
       " 'set': 111,\n",
       " 'alexander': 112,\n",
       " 'quilted': 113,\n",
       " 'of': 114,\n",
       " 'medium': 115,\n",
       " 'tiered': 116,\n",
       " 'stretchjersey': 117,\n",
       " 'row': 118,\n",
       " 'white': 119,\n",
       " 'lipstick': 120,\n",
       " 'isabel': 121,\n",
       " 'oneshoulder': 122,\n",
       " 'loewe': 123,\n",
       " 'mules': 124,\n",
       " 'marant': 125,\n",
       " 'cottonvoile': 126,\n",
       " 'la': 127,\n",
       " 'cardigan': 128,\n",
       " 'swimsuit': 129,\n",
       " 'gathered': 130,\n",
       " 'jersey': 131,\n",
       " 'choo': 132,\n",
       " 'goldtone': 133,\n",
       " 'paneled': 134,\n",
       " 'mcqueen': 135,\n",
       " 'tom': 136,\n",
       " 'ford': 137,\n",
       " 'hoop': 138,\n",
       " 'michael': 139,\n",
       " 'alaa': 140,\n",
       " 'canvas': 141,\n",
       " 'jimmy': 142,\n",
       " 'earring': 143,\n",
       " 'garavani': 144,\n",
       " 'lacetrimmed': 145,\n",
       " 'croceffect': 146,\n",
       " 'lace': 147,\n",
       " 'denim': 148,\n",
       " 'cady': 149,\n",
       " 'offtheshoulder': 150,\n",
       " 'stella': 151,\n",
       " 'halterneck': 152,\n",
       " 'nars': 153,\n",
       " 'slingback': 154,\n",
       " 'mara': 155,\n",
       " 'silkchiffon': 156,\n",
       " 'voile': 157,\n",
       " 'polkadot': 158,\n",
       " 'bracelet': 159,\n",
       " 'cashmereblend': 160,\n",
       " 'dolce': 161,\n",
       " 'mccartney': 162,\n",
       " 'gabbana': 163,\n",
       " 'matte': 164,\n",
       " 'collection': 165,\n",
       " 'victoria': 166,\n",
       " 'givenchy': 167,\n",
       " 'burberry': 168,\n",
       " 'platform': 169,\n",
       " 'van': 170,\n",
       " 'wrapeffect': 171,\n",
       " 'satintrimmed': 172,\n",
       " 'charlotte': 173,\n",
       " 'enamel': 174,\n",
       " 'pussybow': 175,\n",
       " 'max': 176,\n",
       " 'hooded': 177,\n",
       " 'fil': 178,\n",
       " 'coup': 179,\n",
       " 'studios': 180,\n",
       " 'organic': 181,\n",
       " 'nike': 182,\n",
       " 'dries': 183,\n",
       " 'noten': 184,\n",
       " 'distressed': 185,\n",
       " 'silkgeorgette': 186,\n",
       " 'knitted': 187,\n",
       " 'ribbedknit': 188,\n",
       " 'openback': 189,\n",
       " 'tilbury': 190,\n",
       " 'crystal': 191,\n",
       " 'skin': 192,\n",
       " 'le': 193,\n",
       " 'hoodie': 194,\n",
       " 'appliqud': 195,\n",
       " 'linenblend': 196,\n",
       " 'georgette': 197,\n",
       " 'von': 198,\n",
       " 'studded': 199,\n",
       " 'veneta': 200,\n",
       " 'jennifer': 201,\n",
       " 'grosgraintrimmed': 202,\n",
       " 'moncler': 203,\n",
       " 'bottega': 204,\n",
       " 'cardholder': 205,\n",
       " 'snakeeffect': 206,\n",
       " 'fringed': 207,\n",
       " 'cuff': 208,\n",
       " 'ganni': 209,\n",
       " 'silktwill': 210,\n",
       " 'balmain': 211,\n",
       " 'melissa': 212,\n",
       " 'acne': 213,\n",
       " 'multistone': 214,\n",
       " 'diane': 215,\n",
       " 'large': 216,\n",
       " 'fendi': 217,\n",
       " 'zimmermann': 218,\n",
       " 'strapless': 219,\n",
       " 'staud': 220,\n",
       " 'olivia': 221,\n",
       " 'shearling': 222,\n",
       " 'london': 223,\n",
       " 'hair': 224,\n",
       " 'aquazzura': 225,\n",
       " 'alice': 226,\n",
       " 'stretchknit': 227,\n",
       " 'bucket': 228,\n",
       " 'wedge': 229,\n",
       " 'poplin': 230,\n",
       " 'color': 231,\n",
       " 'kors': 232,\n",
       " 'silkcrepe': 233,\n",
       " 'shirred': 234,\n",
       " 'golden': 235,\n",
       " 'resin': 236,\n",
       " 'serum': 237,\n",
       " 'beckham': 238,\n",
       " 'slides': 239,\n",
       " 'bowembellished': 240,\n",
       " 'nili': 241,\n",
       " 'lotan': 242,\n",
       " 'hourglass': 243,\n",
       " 'etro': 244,\n",
       " 'gaia': 245,\n",
       " 'calf': 246,\n",
       " 'jacquard': 247,\n",
       " 'alexandre': 248,\n",
       " 'roland': 249,\n",
       " 'mouret': 250,\n",
       " 'rockstud': 251,\n",
       " 'pvc': 252,\n",
       " 'crocheted': 253,\n",
       " 'down': 254,\n",
       " 'adidas': 255,\n",
       " 'originals': 256,\n",
       " 'silver': 257,\n",
       " 'furstenberg': 258,\n",
       " 'cult': 259,\n",
       " 'gloss': 260,\n",
       " 'liquid': 261,\n",
       " 'broderie': 262,\n",
       " 'marc': 263,\n",
       " 'buttonembellished': 264,\n",
       " 'colorblock': 265,\n",
       " 'knotted': 266,\n",
       " 'merino': 267,\n",
       " 'cream': 268,\n",
       " 'velvettrimmed': 269,\n",
       " 'layered': 270,\n",
       " 'terry': 271,\n",
       " 'see': 272,\n",
       " 'capeeffect': 273,\n",
       " 'zebraprint': 274,\n",
       " 'les': 275,\n",
       " 'spf': 276,\n",
       " 'carolina': 277,\n",
       " 'raffia': 278,\n",
       " 'stick': 279,\n",
       " 'sapphire': 280,\n",
       " 'joseph': 281,\n",
       " 'ulla': 282,\n",
       " 'johnson': 283,\n",
       " 'tweed': 284,\n",
       " 'lacquer': 285,\n",
       " 'silvertone': 286,\n",
       " 'marie': 287,\n",
       " 'camisole': 288,\n",
       " 'glittered': 289,\n",
       " 'goose': 290,\n",
       " 's': 291,\n",
       " 'theory': 292,\n",
       " 'anita': 293,\n",
       " 'silkcharmeuse': 294,\n",
       " 'brown': 295,\n",
       " 'brand': 296,\n",
       " 'oscar': 297,\n",
       " 'jason': 298,\n",
       " 'wu': 299,\n",
       " 'neon': 300,\n",
       " 'renta': 301,\n",
       " 'faithfull': 302,\n",
       " 'vince': 303,\n",
       " 'lou': 304,\n",
       " 'studio': 305,\n",
       " 'houndstooth': 306,\n",
       " 'joy': 307,\n",
       " 'stretchcotton': 308,\n",
       " 'meyer': 309,\n",
       " 'yellow': 310,\n",
       " 'sweatshirt': 311,\n",
       " 'c': 312,\n",
       " 'wooltwill': 313,\n",
       " 'tiedyed': 314,\n",
       " 'eye': 315,\n",
       " 'maison': 316,\n",
       " 'jacobs': 317,\n",
       " 'brunello': 318,\n",
       " 'cucinelli': 319,\n",
       " 'peter': 320,\n",
       " 'proenza': 321,\n",
       " 'schouler': 322,\n",
       " 'anna': 323,\n",
       " 'clip': 324,\n",
       " 'birger': 325,\n",
       " 'manning': 326,\n",
       " 'marni': 327,\n",
       " 'nanushka': 328,\n",
       " 'beaded': 329,\n",
       " 'jacquemus': 330,\n",
       " 'hammeredsatin': 331,\n",
       " 'stretchcrepe': 332,\n",
       " 'ko': 333,\n",
       " 'versace': 334,\n",
       " 'cottontwill': 335,\n",
       " 'nicholas': 336,\n",
       " 'margiela': 337,\n",
       " 'anglaise': 338,\n",
       " 'rebecca': 339,\n",
       " 'norma': 340,\n",
       " 'kaftan': 341,\n",
       " 'co': 342,\n",
       " 'mason': 343,\n",
       " 'lam': 344,\n",
       " 'lane': 345,\n",
       " 'bobbi': 346,\n",
       " 'nude': 347,\n",
       " 'jacquardknit': 348,\n",
       " 'woolcrepe': 349,\n",
       " 'rubber': 350,\n",
       " 'kamali': 351,\n",
       " 'akris': 352,\n",
       " 'air': 353,\n",
       " 'mm': 354,\n",
       " 'loveshackfancy': 355,\n",
       " 'rixo': 356,\n",
       " 'veronica': 357,\n",
       " 'glossedleather': 358,\n",
       " 'tibi': 359,\n",
       " 'chelsea': 360,\n",
       " 'sophia': 361,\n",
       " 'three': 362,\n",
       " 'tiedetailed': 363,\n",
       " 'missoni': 364,\n",
       " 'tiefront': 365,\n",
       " 'shearlingtrimmed': 366,\n",
       " 'crochetknit': 367,\n",
       " 'pinstriped': 368,\n",
       " 'cottongauze': 369,\n",
       " 'beard': 370,\n",
       " 'cableknit': 371,\n",
       " 'vermeil': 372,\n",
       " 'tank': 373,\n",
       " 'phillip': 374,\n",
       " 'lim': 375,\n",
       " 'marmont': 376,\n",
       " 'marchesa': 377,\n",
       " 'vivier': 378,\n",
       " 'mlange': 379,\n",
       " 'sequinembellished': 380,\n",
       " 'frayed': 381,\n",
       " 'love': 382,\n",
       " 'reversible': 383,\n",
       " 'lacepaneled': 384,\n",
       " 'pearlembellished': 385,\n",
       " 'birman': 386,\n",
       " 'no': 387,\n",
       " 'lurex': 388,\n",
       " 'wales': 389,\n",
       " 'cottoncanvas': 390,\n",
       " 'webster': 391,\n",
       " 'gauze': 392,\n",
       " 'line': 393,\n",
       " 'laceup': 394,\n",
       " 'pintucked': 395,\n",
       " 'ear': 396,\n",
       " 'suedetrimmed': 397,\n",
       " 'feathertrimmed': 398,\n",
       " 'suzanne': 399,\n",
       " 'crinkled': 400,\n",
       " 'simone': 401,\n",
       " 'crochettrimmed': 402,\n",
       " 'logoprint': 403,\n",
       " 'shearlinglined': 404,\n",
       " 'silkcrepon': 405,\n",
       " 'galvan': 406,\n",
       " 'kate': 407,\n",
       " 'kalan': 408,\n",
       " 'alex': 409,\n",
       " 'wang': 410,\n",
       " 'lisa': 411,\n",
       " 'hoffman': 412,\n",
       " 'stuart': 413,\n",
       " 'weitzman': 414,\n",
       " 'wandler': 415,\n",
       " 'reformation': 416,\n",
       " 'cecilie': 417,\n",
       " 'bahnsen': 418,\n",
       " 'rosantica': 419,\n",
       " 'far': 420,\n",
       " 'snakeprint': 421,\n",
       " 'surratt': 422,\n",
       " 'rick': 423,\n",
       " 'owens': 424,\n",
       " 'classic': 425,\n",
       " 'stretchsilk': 426,\n",
       " 'thread': 427,\n",
       " 'slub': 428,\n",
       " 'alison': 429,\n",
       " 'fernandez': 430,\n",
       " 'colour': 431,\n",
       " 'flawless': 432,\n",
       " 'salvatore': 433,\n",
       " 'ferragamo': 434,\n",
       " 'kenneth': 435,\n",
       " 'jay': 436,\n",
       " 'yvonne': 437,\n",
       " 'corded': 438,\n",
       " 'sophie': 439,\n",
       " 'palette': 440,\n",
       " 'organza': 441,\n",
       " 'sheer': 442,\n",
       " 'new': 443,\n",
       " 'altuzarra': 444,\n",
       " 'johanna': 445,\n",
       " 'ortiz': 446,\n",
       " 'rouge': 447,\n",
       " 'poudre': 448,\n",
       " 'convertible': 449,\n",
       " 'rocha': 450,\n",
       " 'intarsia': 451,\n",
       " 'paco': 452,\n",
       " 'rabanne': 453,\n",
       " 'gg': 454,\n",
       " 'jcrew': 455,\n",
       " 'finish': 456,\n",
       " 'lee': 457,\n",
       " 'grain': 458,\n",
       " 'emilia': 459,\n",
       " 'pima': 460,\n",
       " 'neoprene': 461,\n",
       " 'quartz': 462,\n",
       " 'chantecaille': 463,\n",
       " 'huda': 464,\n",
       " 'lizardeffect': 465,\n",
       " 'coatedcanvas': 466,\n",
       " 'mcq': 467,\n",
       " 'bowdetailed': 468,\n",
       " 'des': 469,\n",
       " 'lips': 470,\n",
       " 'sterling': 471,\n",
       " 'vanessa': 472,\n",
       " 'khaite': 473,\n",
       " 'shine': 474,\n",
       " 'day': 475,\n",
       " 'tasseled': 476,\n",
       " 'needle': 477,\n",
       " 'bar': 478,\n",
       " 'eyeshadow': 479,\n",
       " 'khan': 480,\n",
       " 'eres': 481,\n",
       " 'double': 482,\n",
       " 'ancient': 483,\n",
       " 'greek': 484,\n",
       " 'shadow': 485,\n",
       " 'gabriela': 486,\n",
       " 'pippa': 487,\n",
       " 'stand': 488,\n",
       " 'stretch': 489,\n",
       " 'cloqu': 490,\n",
       " 'alpacablend': 491,\n",
       " 'wear': 492,\n",
       " 'zoe': 493,\n",
       " 'hearst': 494,\n",
       " 'superstar': 495,\n",
       " 'bone': 496,\n",
       " 'anderson': 497,\n",
       " 'spiked': 498,\n",
       " 'mohairblend': 499,\n",
       " 'offwhite': 500,\n",
       " 'do': 501,\n",
       " 'redvalentino': 502,\n",
       " 'roger': 503,\n",
       " 'sies': 504,\n",
       " 'marjan': 505,\n",
       " 'rejina': 506,\n",
       " 'pyo': 507,\n",
       " 'sacai': 508,\n",
       " 'james': 509,\n",
       " 'patchwork': 510,\n",
       " 'souliers': 511,\n",
       " 'mizuki': 512,\n",
       " 'andrea': 513,\n",
       " 'herrera': 514,\n",
       " 'wickstead': 515,\n",
       " 'aby': 516,\n",
       " 'floraljacquard': 517,\n",
       " 'assoulin': 518,\n",
       " 'christensen': 519,\n",
       " 'comme': 520,\n",
       " 'garons': 521,\n",
       " 'textured': 522,\n",
       " 'roksanda': 523,\n",
       " 'editions': 524,\n",
       " 'scalloped': 525,\n",
       " 'ren': 526,\n",
       " 'loeffler': 527,\n",
       " 'randall': 528,\n",
       " 'peplum': 529,\n",
       " 'beadembellished': 530,\n",
       " 'leset': 531,\n",
       " 'red': 532,\n",
       " 'prince': 533,\n",
       " 'turquoise': 534,\n",
       " 'k': 535,\n",
       " 'rich': 536,\n",
       " 'silkjacquard': 537,\n",
       " 'jil': 538,\n",
       " 'sander': 539,\n",
       " 'rag': 540,\n",
       " 'lorenzo': 541,\n",
       " 'ninety': 542,\n",
       " 'percent': 543,\n",
       " 'saloni': 544,\n",
       " 'boys': 545,\n",
       " 'bernadette': 546,\n",
       " 'onesleeve': 547,\n",
       " 'vauthier': 548,\n",
       " 'ippolita': 549,\n",
       " 'lauren': 550,\n",
       " 'sydney': 551,\n",
       " 'flocked': 552,\n",
       " 'temperley': 553,\n",
       " 'maria': 554,\n",
       " 'buckled': 555,\n",
       " 'lasercut': 556,\n",
       " 'portofino': 557,\n",
       " 'jw': 558,\n",
       " 'este': 559,\n",
       " 'nyc': 560,\n",
       " 'light': 561,\n",
       " 'buttondetailed': 562,\n",
       " 'cefinn': 563,\n",
       " 'high': 564,\n",
       " 'kevyn': 565,\n",
       " 'aucoin': 566,\n",
       " 'genius': 567,\n",
       " 'paulas': 568,\n",
       " 'jenny': 569,\n",
       " 'septembre': 570,\n",
       " 'frame': 571,\n",
       " 'russo': 572,\n",
       " 'dodo': 573,\n",
       " 'or': 574,\n",
       " 'expert': 575,\n",
       " 'stone': 576,\n",
       " 'mugler': 577,\n",
       " 'so': 578,\n",
       " 'erdem': 579,\n",
       " 'evan': 580,\n",
       " 'bucci': 581,\n",
       " 'glow': 582,\n",
       " 'christopher': 583,\n",
       " 'glass': 584,\n",
       " 'wedone': 585,\n",
       " 'thomas': 586,\n",
       " 'opal': 587,\n",
       " 'perse': 588,\n",
       " 'mary': 589,\n",
       " 'logoembellished': 590,\n",
       " 'star': 591,\n",
       " 'fur': 592,\n",
       " 'cami': 593,\n",
       " 'apiece': 594,\n",
       " 'apart': 595,\n",
       " 'atm': 596,\n",
       " 'anthony': 597,\n",
       " 'melillo': 598,\n",
       " 'equipment': 599,\n",
       " 'sleeper': 600,\n",
       " 'silkorganza': 601,\n",
       " 'monse': 602,\n",
       " 'track': 603,\n",
       " 'mother': 604,\n",
       " 'slimleg': 605,\n",
       " 'luxe': 606,\n",
       " 'paris': 607,\n",
       " 'packham': 608,\n",
       " 'preen': 609,\n",
       " 'lauder': 610,\n",
       " 'r': 611,\n",
       " 'vanish': 612,\n",
       " 'seamless': 613,\n",
       " 'caovilla': 614,\n",
       " 'bella': 615,\n",
       " 'glossed': 616,\n",
       " 'ibiza': 617,\n",
       " 'rosetta': 618,\n",
       " 'getty': 619,\n",
       " 'coldshoulder': 620,\n",
       " 'trench': 621,\n",
       " 'espadrille': 622,\n",
       " 'satintwill': 623,\n",
       " 'guipure': 624,\n",
       " 'toile': 625,\n",
       " 'gingham': 626,\n",
       " 'gate': 627,\n",
       " 'chan': 628,\n",
       " 'luu': 629,\n",
       " 'n': 630,\n",
       " 'philosophy': 631,\n",
       " 'di': 632,\n",
       " 'serafini': 633,\n",
       " 'vegan': 634,\n",
       " 'dr': 635,\n",
       " 'tulletrimmed': 636,\n",
       " 'brushed': 637,\n",
       " 'iris': 638,\n",
       " 'cie': 639,\n",
       " 'sock': 640,\n",
       " 'retrofte': 641,\n",
       " 'selfportrait': 642,\n",
       " 'smooth': 643,\n",
       " 'seersucker': 644,\n",
       " 'felt': 645,\n",
       " 'jane': 646,\n",
       " 'bell': 647,\n",
       " 'smocked': 648,\n",
       " 'buccellati': 649,\n",
       " 'vallance': 650,\n",
       " 'charo': 651,\n",
       " 'ruiz': 652,\n",
       " 'moon': 653,\n",
       " 'foundrae': 654,\n",
       " 'notte': 655,\n",
       " 'bille': 656,\n",
       " 'brahe': 657,\n",
       " 'reebok': 658,\n",
       " 'joe': 659,\n",
       " 'ruby': 660,\n",
       " 'lo': 661,\n",
       " 'faille': 662,\n",
       " 'satinjacquard': 663,\n",
       " 'clarita': 664,\n",
       " 'rasario': 665,\n",
       " 'giuseppe': 666,\n",
       " 'zanotti': 667,\n",
       " 'pencil': 668,\n",
       " 'cloe': 669,\n",
       " 'cassandro': 670,\n",
       " 'miguelina': 671,\n",
       " 'valextra': 672,\n",
       " 'rachel': 673,\n",
       " 'queen': 674,\n",
       " 'maggie': 675,\n",
       " 'crepon': 676,\n",
       " 'sea': 677,\n",
       " 'kasha': 678,\n",
       " 'flip': 679,\n",
       " 'pomellato': 680,\n",
       " 'dundas': 681,\n",
       " 'caroline': 682,\n",
       " 'labs': 683,\n",
       " 'mer': 684,\n",
       " 'fohrman': 685,\n",
       " 'perry': 686,\n",
       " 'pushbutton': 687,\n",
       " 'sordo': 688,\n",
       " 'chopard': 689,\n",
       " 'house': 690,\n",
       " 'holland': 691,\n",
       " 'stayinplace': 692,\n",
       " 'makeup': 693,\n",
       " 'heart': 694,\n",
       " 'petar': 695,\n",
       " 'petrov': 696,\n",
       " 'brooke': 697,\n",
       " 'girl': 698,\n",
       " 'marilyn': 699,\n",
       " 'goya': 700,\n",
       " 'kane': 701,\n",
       " 'pilotto': 702,\n",
       " 'ravenel': 703,\n",
       " 'flops': 704,\n",
       " 'loulou': 705,\n",
       " 'robe': 706,\n",
       " 'taffeta': 707,\n",
       " 'harmur': 708,\n",
       " 'talbot': 709,\n",
       " 'runhof': 710,\n",
       " 'loren': 711,\n",
       " 'stewart': 712,\n",
       " 'topaz': 713,\n",
       " 'miller': 714,\n",
       " 'tencel': 715,\n",
       " 'bomber': 716,\n",
       " 'nubuck': 717,\n",
       " 'malene': 718,\n",
       " 'safiyaa': 719,\n",
       " 'pink': 720,\n",
       " 'happy': 721,\n",
       " 'alessandra': 722,\n",
       " 'nina': 723,\n",
       " 'chainembellished': 724,\n",
       " 'orseund': 725,\n",
       " 'envelope': 726,\n",
       " 'veja': 727,\n",
       " 'pat': 728,\n",
       " 'mcgrath': 729,\n",
       " 'tory': 730,\n",
       " 'burch': 731,\n",
       " 'pascale': 732,\n",
       " 'monvoisin': 733,\n",
       " 'adam': 734,\n",
       " 'lippes': 735,\n",
       " 'shorts': 736,\n",
       " 'halle': 737,\n",
       " 'belle': 738,\n",
       " 'camilla': 739,\n",
       " 'brocade': 740,\n",
       " 'cushnie': 741,\n",
       " 'spinelli': 742,\n",
       " 'kilcollin': 743,\n",
       " 'stine': 744,\n",
       " 'twenty': 745,\n",
       " 'montral': 746,\n",
       " 'tess': 747,\n",
       " 'choker': 748,\n",
       " 'andersson': 749,\n",
       " 'kassl': 750,\n",
       " 'ruffletrimmed': 751,\n",
       " 'attico': 752,\n",
       " 'boucl': 753,\n",
       " 'et': 754,\n",
       " 'elder': 755,\n",
       " 'statesman': 756,\n",
       " 'redone': 757,\n",
       " 'oil': 758,\n",
       " 'joan': 759,\n",
       " 'lvres': 760,\n",
       " 'tieneck': 761,\n",
       " 'rveries': 762,\n",
       " 'solid': 763,\n",
       " 'motherofpearl': 764,\n",
       " 'kjaer': 765,\n",
       " 'weis': 766,\n",
       " 'face': 767,\n",
       " 'michelle': 768,\n",
       " 'vetements': 769,\n",
       " 'airbrush': 770,\n",
       " 'onyx': 771,\n",
       " 'eyes': 772,\n",
       " 'makri': 773,\n",
       " 'tapered': 774,\n",
       " 'girls': 775,\n",
       " 'rotate': 776,\n",
       " 'hammered': 777,\n",
       " 'paisleyprint': 778,\n",
       " 'stretchvelvet': 779,\n",
       " 'brandon': 780,\n",
       " 'emerald': 781,\n",
       " 'animalprint': 782,\n",
       " 'atelier': 783,\n",
       " 'ellery': 784,\n",
       " 'warm': 785,\n",
       " 'luna': 786,\n",
       " 'paul': 787,\n",
       " 'a': 788,\n",
       " 'rosie': 789,\n",
       " 'naeem': 790,\n",
       " 'hot': 791,\n",
       " 'maxwell': 792,\n",
       " 'rubbertrimmed': 793,\n",
       " 'agnona': 794,\n",
       " 'khouri': 795,\n",
       " 'malone': 796,\n",
       " 'full': 797,\n",
       " 'ralph': 798,\n",
       " 'exaggeratedsole': 799,\n",
       " 'moschino': 800,\n",
       " 'sant': 801,\n",
       " 'bassike': 802,\n",
       " 'loro': 803,\n",
       " 'piana': 804,\n",
       " 'ramie': 805,\n",
       " 'beige': 806,\n",
       " 'zumi': 807,\n",
       " 'ann': 808,\n",
       " 'demeulemeester': 809,\n",
       " 'escape': 810,\n",
       " 'bead': 811,\n",
       " 'corduroy': 812,\n",
       " 'diana': 813,\n",
       " 'rings': 814,\n",
       " 'matriel': 815,\n",
       " 'solace': 816,\n",
       " 'concentrate': 817,\n",
       " 'chain': 818,\n",
       " 'intrecciato': 819,\n",
       " 'kaye': 820,\n",
       " 'super': 821,\n",
       " 'roxanne': 822,\n",
       " 'graces': 823,\n",
       " 'cottongabardine': 824,\n",
       " 'rails': 825,\n",
       " 'swissdot': 826,\n",
       " 'ashish': 827,\n",
       " 'x': 828,\n",
       " 'furtrimmed': 829,\n",
       " 'react': 830,\n",
       " 'tash': 831,\n",
       " 'maticevski': 832,\n",
       " 'honey': 833,\n",
       " 'lust': 834,\n",
       " 'valli': 835,\n",
       " 'flannel': 836,\n",
       " 'katrantzou': 837,\n",
       " 'lon': 838,\n",
       " 'molly': 839,\n",
       " 'kirkwood': 840,\n",
       " 'aj': 841,\n",
       " 'rta': 842,\n",
       " 'bracelets': 843,\n",
       " 'natural': 844,\n",
       " 'illamasqua': 845,\n",
       " 'ophidia': 846,\n",
       " 'anglaisetrimmed': 847,\n",
       " 'moisturizer': 848,\n",
       " 'culottes': 849,\n",
       " 'munthe': 850,\n",
       " 'baby': 851,\n",
       " 'mill': 852,\n",
       " 'kayu': 853,\n",
       " 'satincrepe': 854,\n",
       " 'alighieri': 855,\n",
       " 'low': 856,\n",
       " 'thornton': 857,\n",
       " 'bregazzi': 858,\n",
       " 'barbara': 859,\n",
       " 'for': 860,\n",
       " 'racil': 861,\n",
       " 'vinyl': 862,\n",
       " 'kenzo': 863,\n",
       " 'raquel': 864,\n",
       " 'awake': 865,\n",
       " 'mode': 866,\n",
       " 'eberjey': 867,\n",
       " 'xs': 868,\n",
       " 'amber': 869,\n",
       " 'padded': 870,\n",
       " 'tl': 871,\n",
       " 'mathews': 872,\n",
       " 'perforated': 873,\n",
       " 'aeyd': 874,\n",
       " 'strand': 875,\n",
       " 'gregson': 876,\n",
       " 'constas': 877,\n",
       " 't': 878,\n",
       " 'paradised': 879,\n",
       " 'g': 880,\n",
       " 'art': 881,\n",
       " 'sylva': 882,\n",
       " 'stretchmesh': 883,\n",
       " 'hyaluronic': 884,\n",
       " 'mansur': 885,\n",
       " 'gavriel': 886,\n",
       " 'twistfront': 887,\n",
       " 'flyknit': 888,\n",
       " 'brock': 889,\n",
       " 'bite': 890,\n",
       " 'goddard': 891,\n",
       " 'range': 892,\n",
       " 'romy': 893,\n",
       " 'monica': 894,\n",
       " 'city': 895,\n",
       " 'stretchcady': 896,\n",
       " 'marques': 897,\n",
       " 'almeida': 898,\n",
       " 'ana': 899,\n",
       " 'silktaffeta': 900,\n",
       " 'iro': 901,\n",
       " 'common': 902,\n",
       " 'projects': 903,\n",
       " 'halpern': 904,\n",
       " 'vali': 905,\n",
       " 'cabas': 906,\n",
       " 'wander': 907,\n",
       " 'cottoncorduroy': 908,\n",
       " 'emma': 909,\n",
       " 'on': 910,\n",
       " 'totme': 911,\n",
       " 'andrew': 912,\n",
       " 'gn': 913,\n",
       " 'tods': 914,\n",
       " 'hearts': 915,\n",
       " 'herv': 916,\n",
       " 'lger': 917,\n",
       " 'selim': 918,\n",
       " 'mouzannar': 919,\n",
       " 'patent': 920,\n",
       " 'long': 921,\n",
       " 'forteforte': 922,\n",
       " 'coco': 923,\n",
       " 'dion': 924,\n",
       " 'triple': 925,\n",
       " 'acrylic': 926,\n",
       " 'haider': 927,\n",
       " 'ackermann': 928,\n",
       " 'ole': 929,\n",
       " 'lynggaard': 930,\n",
       " 'copenhagen': 931,\n",
       " 'hvn': 932,\n",
       " 'lori': 933,\n",
       " 'silvia': 934,\n",
       " 'chiffontrimmed': 935,\n",
       " 'vanina': 936,\n",
       " 'giambattista': 937,\n",
       " 'sally': 938,\n",
       " 'lapointe': 939,\n",
       " 'ambush': 940,\n",
       " 'georgia': 941,\n",
       " 'cocchiaro': 942,\n",
       " 'plissgeorgette': 943,\n",
       " 'morgan': 944,\n",
       " 'lola': 945,\n",
       " 'versilia': 946,\n",
       " 'fisher': 947,\n",
       " 'sisley': 948,\n",
       " 'lfmarkey': 949,\n",
       " 'vinader': 950,\n",
       " 'candy': 951,\n",
       " 'herringbone': 952,\n",
       " 'leigh': 953,\n",
       " 'contour': 954,\n",
       " 'boucheron': 955,\n",
       " 'neous': 956,\n",
       " 'straw': 957,\n",
       " 'crme': 958,\n",
       " 'power': 959,\n",
       " 'evi': 960,\n",
       " 'grintela': 961,\n",
       " 'dgrad': 962,\n",
       " 'heidi': 963,\n",
       " 'klein': 964,\n",
       " 'i': 965,\n",
       " 'du': 966,\n",
       " 'repossi': 967,\n",
       " 'bruno': 968,\n",
       " 'fluted': 969,\n",
       " 'larkspur': 970,\n",
       " 'hawk': 971,\n",
       " 'ultra': 972,\n",
       " 'pareo': 973,\n",
       " 'bustier': 974,\n",
       " 'palmerharding': 975,\n",
       " 'cover': 976,\n",
       " 'madewell': 977,\n",
       " 'lagence': 978,\n",
       " 'kimberly': 979,\n",
       " 'mcdonald': 980,\n",
       " 'cottonvelvet': 981,\n",
       " 'sorel': 982,\n",
       " 'soleil': 983,\n",
       " 'gel': 984,\n",
       " 'sturm': 985,\n",
       " 'b': 986,\n",
       " 'bangle': 987,\n",
       " 'deitas': 988,\n",
       " 'enamored': 989,\n",
       " 'sun': 990,\n",
       " 'vapormax': 991,\n",
       " 'soft': 992,\n",
       " 'shade': 993,\n",
       " 'fair': 994,\n",
       " 'rare': 995,\n",
       " 'origin': 996,\n",
       " 'hunting': 997,\n",
       " 'season': 998,\n",
       " 'vlogo': 999,\n",
       " 'lily': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the tokens\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f1oat1rU7LRn",
    "outputId": "b58ee95c-5371-4c93-854b-0db722589cd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (65714, 15)\n"
     ]
    }
   ],
   "source": [
    "## Finally we are creating our text data for training\n",
    "X = tokenizer.texts_to_sequences(names) # Here it takes each word from names and replaces it with it's corresponding number from our word_index\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH) # it transforms list of sequences into numpy aray of shape (num_sequence,num_timestep) \n",
    "## Where num sequence is number of sentences in names and num_timestep is number of words in each sentence which is 15 in this case.\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "jbY0coqq7LPX",
    "outputId": "e12ab32d-3583-4d82-af2d-8986c691763a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 2634,  222,   69],\n",
       "       [   0,    0,    0, ...,    8,   53,  101],\n",
       "       [   0,    0,    0, ...,   10,  332,   72],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    3,  350,   33],\n",
       "       [   0,    0,    0, ...,  199,   14,   11],\n",
       "       [   0,    0,    0, ...,    2,  228,   13]], dtype=int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can see here an array where for every example(sentence) we have 15 fetaures(words).\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "UV05tnokClu-",
    "outputId": "02888612-ab7e-4d10-cfb4-7d0e77811287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 15) (60000, 48)\n",
      "(5714, 15) (5714, 48)\n"
     ]
    }
   ],
   "source": [
    "## Now let's create train and test datasets\n",
    "X_traintext=X[0:60000,:]\n",
    "X_testtext=X[60000:,:]\n",
    "print(X_traintext.shape,Y_train.shape)\n",
    "print(X_testtext.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QAzkbfi8g8ji"
   },
   "source": [
    "Saving preprocessesd text data for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJjQ_czTPsrV"
   },
   "outputs": [],
   "source": [
    "#save('/content/drive/My Drive/train_text.npy',X_traintext)\n",
    "#save('/content/drive/My Drive/test_text.npy',X_testtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j6FSNR9bhB-d"
   },
   "source": [
    "We can reload the text data by using below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4VmaEtqCPJ4"
   },
   "outputs": [],
   "source": [
    "X_traintext=load('/content/drive/My Drive/data ecommerce /train_text.npy')\n",
    "X_testtext=load('/content/drive/My Drive/data ecommerce /test_text.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rORm_ZQzVd1b"
   },
   "source": [
    "# **Image Embeddings**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9px8jsIPhlaL"
   },
   "source": [
    "This is part 1 of the assignment where we will use VGG16 pretrained model to get image embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "n-XEUjys4-0Y",
    "outputId": "585aa329-2b6e-4175-ffe4-5e98167e0b84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "new_model = VGG16(weights='imagenet',include_top=False,input_shape=(64,64,3)) ## Loading VGG16 model with shape(64,64,3), \n",
    "## Excluding last layer and using imagenet weights.\n",
    "model=Sequential()       ## Creating new model where we will load our vgg16 model\n",
    "for layer in new_model.layers:\n",
    "  model.add(layer)\n",
    "for layer in model.layers: ## Setting layers of model to trianable as false\n",
    "  layer.trainable=False\n",
    "model.add(Flatten())       ## Adding a Flatten layer at the end to get a vector output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "colab_type": "code",
    "id": "2c_Plbpe5cKs",
    "outputId": "69e43a39-62ea-4958-9b07-f0280690d667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Now let's look at the summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_e4B4TWmpIW"
   },
   "source": [
    "You can see Flatten layer at the end which outputs a 2048 dimensional vector. These 2048 embeddings will be passed to image tower later. Talking about the model you can see there are 14,714,688 total parameters but trainable parameters are 0 as wee don't want to train VGG16 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "inlzz2kM53A2"
   },
   "source": [
    "# **Implementinng Image Tower**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m4HLtXtDnpg1"
   },
   "source": [
    "Now we will implement part 2 image tower. Here we have taken refernce from \"4.1.3 Training Process\" of paper to build image tower. We have also taken refernce from \"3.2. Network Structure\" third paragraph Which is \"The text tower and image tower can have different number of hidden layers and different hidden layer sizes. The only restriction is that the last layer of both towers should have D hidden units followed by L2 normalization\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f6Pq96Sl6Ug5"
   },
   "outputs": [],
   "source": [
    "## Creating image_tower model\n",
    "## As defined in paper we are going to use 5 hidden Dense layer with 512 units each\n",
    "## A dropout of 0.15 is used between all hidden layers.\n",
    "## L2 normalization is used \n",
    "image_tower=Sequential()\n",
    "image_tower.add(Dense(512,activation='softmax'))\n",
    "image_tower.add(Dropout(0.15))\n",
    "image_tower.add(Dense(512,activation='softmax'))\n",
    "image_tower.add(Dropout(0.15))\n",
    "image_tower.add(Dense(512,activation='softmax'))\n",
    "image_tower.add(Dropout(0.15))\n",
    "image_tower.add(Dense(512,activation='softmax'))\n",
    "image_tower.add(Dropout(0.15))\n",
    "image_tower.add(Dense(512,activation='softmax'))   ## Last layer with D=512 hidden units followed by L2 normalization.\n",
    "image_tower.add(Lambda(lambda xz:K.l2_normalize(xz,axis=1))) ## Normalizing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cPVQfDT1sBt2"
   },
   "source": [
    "Now we will add our image_tower model to image embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "etwIAzxv6cV2"
   },
   "outputs": [],
   "source": [
    "model.add(image_tower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 823
    },
    "colab_type": "code",
    "id": "4nCGVj1L6cRs",
    "outputId": "05d13bc5-08e7-4ee6-c5e0-d3b1476805a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 512)               2099712   \n",
      "=================================================================\n",
      "Total params: 16,814,400\n",
      "Trainable params: 2,099,712\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XOZEVbdhsnDv"
   },
   "source": [
    "As you can see there are 14,714,688 non-trainable parameters from previous model and 2,099,712 new parameters from image_tower model which are trainable. Last layer 'sequential_2' represents image_tower model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3NJ8N_HVz7L_"
   },
   "source": [
    "# **Text Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PHWMW9G8tvef"
   },
   "source": [
    "Part 1 text embedding. Here we have decided to train a small model for text embedding rather than using pretrained model. This model will be traind and it will pass text embeddings to text_tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aROucZEhVVuA"
   },
   "outputs": [],
   "source": [
    "## Creating textmodel \n",
    "textmodel=Sequential()\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "textmodel.add(Embedding(MAX_NB_WORDS,\n",
    "                    EMBEDDING_DIM,input_length=X_traintext.shape[1]))\n",
    "textmodel.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "textmodel.add(Conv1D(256,\n",
    "                 5,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "textmodel.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a Fully Connected hidden layer:\n",
    "textmodel.add(Dense(512))\n",
    "textmodel.add(Dropout(0.2))\n",
    "textmodel.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "5uf_igPnsOyW",
    "outputId": "946cf4cb-7dc9-41a7-d483-72bec6980f80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 15, 100)           500000    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 15, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 11, 256)           128256    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 759,840\n",
      "Trainable params: 759,840\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Now look at the summary of this model\n",
    "textmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zlnlj1zhuici"
   },
   "source": [
    "So we have 7 layers in this model and 759,840 parameters which are trainable. And last layer will pass text embeddings to text tower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XvPYcvTo7LeW"
   },
   "source": [
    "# **Implementing Text tower**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CPuwWtl-vD2s"
   },
   "source": [
    "Now we will implement part 2 text tower. Here we have taken refernce from \"4.1.3 Training Process\" of paper to build text tower. We have also taken refernce from \"3.2. Network Structure\" third paragraph Which is \"The text tower and image tower can have different number of hidden layers and different hidden layer sizes. The only restriction is that the last layer of both towers should have D hidden units followed by L2 normalization\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HiHXYSCz7P4y"
   },
   "outputs": [],
   "source": [
    "## Creating text_tower model\n",
    "## As defined in paper we are going to use 2 hidden Dense layer with 512 units each\n",
    "## A dropout of 0.15 is used between all hidden layers.\n",
    "## L2 normalization is used\n",
    "text_tower=Sequential()\n",
    "text_tower.add(Dense(512,activation='relu'))\n",
    "text_tower.add(Dropout(0.15))\n",
    "text_tower.add(Dense(512,activation='relu'))  ## Last layer with D=512 hidden units followed by L2 normalization.\n",
    "text_tower.add(Lambda(lambda xz:K.l2_normalize(xz,axis=1))) ## L2 normalizaton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSj-r1Quv3iH"
   },
   "source": [
    "Now we will add text_tower model to text embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oche1vLK7ici"
   },
   "outputs": [],
   "source": [
    "## Adding text_tower to textmodel\n",
    "textmodel.add(text_tower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "colab_type": "code",
    "id": "77JZg7ic7m69",
    "outputId": "26088853-8075-4a7a-f872-3e55abd27b40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 15, 100)           500000    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 15, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 11, 256)           128256    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "sequential_4 (Sequential)    (None, 512)               525312    \n",
      "=================================================================\n",
      "Total params: 1,285,152\n",
      "Trainable params: 1,285,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Let's see textmodel now.\n",
    "textmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hiQMfy6gw2xy"
   },
   "source": [
    "As you can see there are total 1,285,152 parameters and all parameters are set to be trainable. The last layer which is 'sequential_4' represents text_tower model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6IQvXBa8UGh"
   },
   "source": [
    "# **Final Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bmiiMJq6zp72"
   },
   "source": [
    "Now we will merge both the models and add final layer for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJ6ve2Mw8hZ5"
   },
   "outputs": [],
   "source": [
    "merged=concatenate([model.output,textmodel.output]) ## Merging both the models\n",
    "out=Dense(48)(merged) ## Adding a Dense layer to merged layers\n",
    "out=Activation('softmax')(out) ## Adding activation softmax for classification\n",
    "final_model=Model([model.input,textmodel.input],out) ## Final model which we will use for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lIrZqDCR8mdW",
    "outputId": "44eec9d6-6707-43d6-8af1-cba2d667f299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 64, 64, 64)   1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 64, 64, 64)   36928       block1_conv1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 32, 32, 64)   0           block1_conv2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 32, 32, 128)  73856       block1_pool[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 32, 32, 128)  147584      block2_conv1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 16, 16, 128)  0           block2_conv2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 16, 16, 256)  295168      block2_pool[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 16, 16, 256)  590080      block3_conv1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 16, 16, 256)  590080      block3_conv2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 8, 8, 256)    0           block3_conv3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 8, 8, 512)    1180160     block3_pool[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 8, 8, 512)    2359808     block4_conv1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1_input (InputLayer)  (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 8, 8, 512)    2359808     block4_conv2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 15, 100)      500000      embedding_1_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 4, 4, 512)    0           block4_conv3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 15, 100)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 4, 4, 512)    2359808     block4_pool[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 11, 256)      128256      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 4, 4, 512)    2359808     block5_conv1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 4, 4, 512)    2359808     block5_conv2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 512)          131584      global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, 2, 2, 512)    0           block5_conv3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 512)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2048)         0           block5_pool[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 512)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 512)          2099712     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 512)          525312      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1024)         0           sequential_2[1][0]               \n",
      "                                                                 sequential_4[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 48)           49200       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 48)           0           dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 18,148,752\n",
      "Trainable params: 3,434,064\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Let's have a look at the final model.\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80WIZ4gc0Z1x"
   },
   "source": [
    "Look at the 'sequential_2' layer it is the output from image_tower simillarly 'sequential_4' is the output from text_tower. These layers are merged(concatenated) into 'concatenate_1' layer. In final_model we have a total of 18,148,752 parameters out of which onlt 3,434,064 are trainable. 14,714,688 parameters are non-trainable which are from VGG16 model. This model will be used further for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KZWT8etI1zxv"
   },
   "source": [
    "# **Class Level Similarity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OifuiO2o2Be_"
   },
   "source": [
    "As defined in \"4.1.3 Training Process\" we will use RMSprop optimization with learning_rate=1.6192e-5. We will use \"categorical_crossentropy\" as written in \"3.4.1 Class Level Similarity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjVC9kSn8o5Q"
   },
   "outputs": [],
   "source": [
    "final_model.compile(optimizer=optimizers.RMSprop(lr=1.6192e-5),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fU_gAY22wFu"
   },
   "source": [
    "We are definining a callback which will stop training after we have achieved the accuracy of 98% or greater on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I218npUOEX4v"
   },
   "outputs": [],
   "source": [
    "class myCallback(keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('val_accuracy')>=0.98):\n",
    "      print(\"\\n Reached accuracy of 98% hence stopping training\")\n",
    "      self.model.stop_training=True\n",
    "callbacks=myCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xkt3vMuy3ZLU"
   },
   "source": [
    "Finally training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qVyRLQlc89ZT",
    "outputId": "d66f7cf4-006c-4d64-b461-58eee4eb09d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57000 samples, validate on 3000 samples\n",
      "Epoch 1/200\n",
      "57000/57000 [==============================] - 18s 311us/step - loss: 3.8259 - accuracy: 0.0569 - val_loss: 3.7845 - val_accuracy: 0.0853\n",
      "Epoch 2/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 3.7592 - accuracy: 0.0821 - val_loss: 3.7192 - val_accuracy: 0.1197\n",
      "Epoch 3/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 3.6862 - accuracy: 0.1555 - val_loss: 3.6391 - val_accuracy: 0.2067\n",
      "Epoch 4/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 3.6043 - accuracy: 0.2172 - val_loss: 3.5566 - val_accuracy: 0.2500\n",
      "Epoch 5/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 3.5209 - accuracy: 0.2676 - val_loss: 3.4723 - val_accuracy: 0.3177\n",
      "Epoch 6/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 3.4358 - accuracy: 0.3297 - val_loss: 3.3866 - val_accuracy: 0.3663\n",
      "Epoch 7/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 3.3522 - accuracy: 0.3822 - val_loss: 3.3044 - val_accuracy: 0.4060\n",
      "Epoch 8/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 3.2736 - accuracy: 0.4167 - val_loss: 3.2283 - val_accuracy: 0.4357\n",
      "Epoch 9/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 3.2010 - accuracy: 0.4366 - val_loss: 3.1577 - val_accuracy: 0.4457\n",
      "Epoch 10/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 3.1327 - accuracy: 0.4471 - val_loss: 3.0913 - val_accuracy: 0.4573\n",
      "Epoch 11/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 3.0694 - accuracy: 0.4550 - val_loss: 3.0302 - val_accuracy: 0.4640\n",
      "Epoch 12/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 3.0102 - accuracy: 0.4608 - val_loss: 2.9720 - val_accuracy: 0.4687\n",
      "Epoch 13/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 2.9537 - accuracy: 0.4645 - val_loss: 2.9171 - val_accuracy: 0.4767\n",
      "Epoch 14/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 2.8997 - accuracy: 0.4724 - val_loss: 2.8631 - val_accuracy: 0.4830\n",
      "Epoch 15/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 2.8471 - accuracy: 0.4800 - val_loss: 2.8124 - val_accuracy: 0.4947\n",
      "Epoch 16/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 2.7961 - accuracy: 0.4894 - val_loss: 2.7623 - val_accuracy: 0.5097\n",
      "Epoch 17/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 2.7460 - accuracy: 0.5126 - val_loss: 2.7138 - val_accuracy: 0.5447\n",
      "Epoch 18/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 2.6964 - accuracy: 0.5523 - val_loss: 2.6652 - val_accuracy: 0.5787\n",
      "Epoch 19/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 2.6476 - accuracy: 0.5788 - val_loss: 2.6166 - val_accuracy: 0.5990\n",
      "Epoch 20/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 2.5992 - accuracy: 0.6035 - val_loss: 2.5680 - val_accuracy: 0.6253\n",
      "Epoch 21/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 2.5508 - accuracy: 0.6310 - val_loss: 2.5222 - val_accuracy: 0.6453\n",
      "Epoch 22/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 2.5029 - accuracy: 0.6493 - val_loss: 2.4756 - val_accuracy: 0.6557\n",
      "Epoch 23/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 2.4557 - accuracy: 0.6653 - val_loss: 2.4283 - val_accuracy: 0.6677\n",
      "Epoch 24/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 2.4085 - accuracy: 0.6767 - val_loss: 2.3839 - val_accuracy: 0.6800\n",
      "Epoch 25/200\n",
      "57000/57000 [==============================] - 16s 278us/step - loss: 2.3621 - accuracy: 0.6855 - val_loss: 2.3360 - val_accuracy: 0.6907\n",
      "Epoch 26/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 2.3159 - accuracy: 0.6930 - val_loss: 2.2910 - val_accuracy: 0.6953\n",
      "Epoch 27/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 2.2701 - accuracy: 0.7038 - val_loss: 2.2466 - val_accuracy: 0.7073\n",
      "Epoch 28/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 2.2244 - accuracy: 0.7119 - val_loss: 2.2017 - val_accuracy: 0.7157\n",
      "Epoch 29/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 2.1797 - accuracy: 0.7159 - val_loss: 2.1575 - val_accuracy: 0.7193\n",
      "Epoch 30/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 2.1351 - accuracy: 0.7198 - val_loss: 2.1143 - val_accuracy: 0.7190\n",
      "Epoch 31/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 2.0910 - accuracy: 0.7283 - val_loss: 2.0692 - val_accuracy: 0.7347\n",
      "Epoch 32/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 2.0473 - accuracy: 0.7347 - val_loss: 2.0278 - val_accuracy: 0.7357\n",
      "Epoch 33/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 2.0043 - accuracy: 0.7422 - val_loss: 1.9845 - val_accuracy: 0.7470\n",
      "Epoch 34/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 1.9617 - accuracy: 0.7492 - val_loss: 1.9438 - val_accuracy: 0.7517\n",
      "Epoch 35/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 1.9197 - accuracy: 0.7575 - val_loss: 1.9029 - val_accuracy: 0.7563\n",
      "Epoch 36/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 1.8782 - accuracy: 0.7656 - val_loss: 1.8612 - val_accuracy: 0.7633\n",
      "Epoch 37/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 1.8373 - accuracy: 0.7711 - val_loss: 1.8226 - val_accuracy: 0.7703\n",
      "Epoch 38/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 1.7972 - accuracy: 0.7765 - val_loss: 1.7820 - val_accuracy: 0.7757\n",
      "Epoch 39/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 1.7574 - accuracy: 0.7808 - val_loss: 1.7433 - val_accuracy: 0.7803\n",
      "Epoch 40/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 1.7179 - accuracy: 0.7859 - val_loss: 1.7025 - val_accuracy: 0.7880\n",
      "Epoch 41/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 1.6792 - accuracy: 0.7928 - val_loss: 1.6645 - val_accuracy: 0.7893\n",
      "Epoch 42/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 1.6407 - accuracy: 0.7990 - val_loss: 1.6277 - val_accuracy: 0.8013\n",
      "Epoch 43/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 1.6033 - accuracy: 0.8064 - val_loss: 1.5904 - val_accuracy: 0.8063\n",
      "Epoch 44/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 1.5658 - accuracy: 0.8129 - val_loss: 1.5542 - val_accuracy: 0.8157\n",
      "Epoch 45/200\n",
      "57000/57000 [==============================] - 16s 278us/step - loss: 1.5293 - accuracy: 0.8186 - val_loss: 1.5195 - val_accuracy: 0.8180\n",
      "Epoch 46/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 1.4935 - accuracy: 0.8234 - val_loss: 1.4857 - val_accuracy: 0.8200\n",
      "Epoch 47/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 1.4579 - accuracy: 0.8271 - val_loss: 1.4481 - val_accuracy: 0.8283\n",
      "Epoch 48/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 1.4226 - accuracy: 0.8320 - val_loss: 1.4165 - val_accuracy: 0.8280\n",
      "Epoch 49/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 1.3885 - accuracy: 0.8357 - val_loss: 1.3844 - val_accuracy: 0.8357\n",
      "Epoch 50/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 1.3543 - accuracy: 0.8398 - val_loss: 1.3505 - val_accuracy: 0.8353\n",
      "Epoch 51/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 1.3218 - accuracy: 0.8415 - val_loss: 1.3156 - val_accuracy: 0.8387\n",
      "Epoch 52/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 1.2890 - accuracy: 0.8438 - val_loss: 1.2863 - val_accuracy: 0.8413\n",
      "Epoch 53/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 1.2573 - accuracy: 0.8459 - val_loss: 1.2529 - val_accuracy: 0.8437\n",
      "Epoch 54/200\n",
      "57000/57000 [==============================] - 16s 279us/step - loss: 1.2257 - accuracy: 0.8480 - val_loss: 1.2236 - val_accuracy: 0.8477\n",
      "Epoch 55/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 1.1954 - accuracy: 0.8510 - val_loss: 1.1957 - val_accuracy: 0.8480\n",
      "Epoch 56/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 1.1650 - accuracy: 0.8534 - val_loss: 1.1629 - val_accuracy: 0.8490\n",
      "Epoch 57/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 1.1355 - accuracy: 0.8559 - val_loss: 1.1337 - val_accuracy: 0.8513\n",
      "Epoch 58/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 1.1072 - accuracy: 0.8584 - val_loss: 1.1079 - val_accuracy: 0.8537\n",
      "Epoch 59/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 1.0783 - accuracy: 0.8621 - val_loss: 1.0840 - val_accuracy: 0.8527\n",
      "Epoch 60/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 1.0503 - accuracy: 0.8646 - val_loss: 1.0556 - val_accuracy: 0.8580\n",
      "Epoch 61/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 1.0235 - accuracy: 0.8655 - val_loss: 1.0300 - val_accuracy: 0.8557\n",
      "Epoch 62/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.9973 - accuracy: 0.8666 - val_loss: 1.0047 - val_accuracy: 0.8583\n",
      "Epoch 63/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.9716 - accuracy: 0.8676 - val_loss: 0.9783 - val_accuracy: 0.8590\n",
      "Epoch 64/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.9460 - accuracy: 0.8682 - val_loss: 0.9551 - val_accuracy: 0.8583\n",
      "Epoch 65/200\n",
      "57000/57000 [==============================] - 16s 278us/step - loss: 0.9216 - accuracy: 0.8691 - val_loss: 0.9284 - val_accuracy: 0.8607\n",
      "Epoch 66/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.8972 - accuracy: 0.8698 - val_loss: 0.9077 - val_accuracy: 0.8607\n",
      "Epoch 67/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.8745 - accuracy: 0.8698 - val_loss: 0.8846 - val_accuracy: 0.8603\n",
      "Epoch 68/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.8511 - accuracy: 0.8707 - val_loss: 0.8646 - val_accuracy: 0.8593\n",
      "Epoch 69/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.8294 - accuracy: 0.8712 - val_loss: 0.8444 - val_accuracy: 0.8610\n",
      "Epoch 70/200\n",
      "57000/57000 [==============================] - 16s 274us/step - loss: 0.8081 - accuracy: 0.8718 - val_loss: 0.8219 - val_accuracy: 0.8647\n",
      "Epoch 71/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.7865 - accuracy: 0.8739 - val_loss: 0.7992 - val_accuracy: 0.8680\n",
      "Epoch 72/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.7664 - accuracy: 0.8762 - val_loss: 0.7802 - val_accuracy: 0.8707\n",
      "Epoch 73/200\n",
      "57000/57000 [==============================] - 16s 279us/step - loss: 0.7460 - accuracy: 0.8804 - val_loss: 0.7607 - val_accuracy: 0.8753\n",
      "Epoch 74/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.7265 - accuracy: 0.8838 - val_loss: 0.7425 - val_accuracy: 0.8760\n",
      "Epoch 75/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.7076 - accuracy: 0.8870 - val_loss: 0.7218 - val_accuracy: 0.8800\n",
      "Epoch 76/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.6891 - accuracy: 0.8890 - val_loss: 0.7056 - val_accuracy: 0.8810\n",
      "Epoch 77/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.6711 - accuracy: 0.8905 - val_loss: 0.6872 - val_accuracy: 0.8823\n",
      "Epoch 78/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.6542 - accuracy: 0.8918 - val_loss: 0.6704 - val_accuracy: 0.8850\n",
      "Epoch 79/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.6368 - accuracy: 0.8931 - val_loss: 0.6520 - val_accuracy: 0.8847\n",
      "Epoch 80/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.6204 - accuracy: 0.8937 - val_loss: 0.6366 - val_accuracy: 0.8877\n",
      "Epoch 81/200\n",
      "57000/57000 [==============================] - 16s 274us/step - loss: 0.6042 - accuracy: 0.8944 - val_loss: 0.6225 - val_accuracy: 0.8867\n",
      "Epoch 82/200\n",
      "57000/57000 [==============================] - 16s 274us/step - loss: 0.5882 - accuracy: 0.8961 - val_loss: 0.6064 - val_accuracy: 0.8883\n",
      "Epoch 83/200\n",
      "57000/57000 [==============================] - 16s 274us/step - loss: 0.5726 - accuracy: 0.8975 - val_loss: 0.5902 - val_accuracy: 0.8883\n",
      "Epoch 84/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.5575 - accuracy: 0.8989 - val_loss: 0.5788 - val_accuracy: 0.8917\n",
      "Epoch 85/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.5433 - accuracy: 0.9018 - val_loss: 0.5623 - val_accuracy: 0.8970\n",
      "Epoch 86/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.5297 - accuracy: 0.9044 - val_loss: 0.5491 - val_accuracy: 0.9003\n",
      "Epoch 87/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.5151 - accuracy: 0.9077 - val_loss: 0.5381 - val_accuracy: 0.8993\n",
      "Epoch 88/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.5026 - accuracy: 0.9103 - val_loss: 0.5264 - val_accuracy: 0.9013\n",
      "Epoch 89/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.4887 - accuracy: 0.9131 - val_loss: 0.5104 - val_accuracy: 0.9070\n",
      "Epoch 90/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.4763 - accuracy: 0.9145 - val_loss: 0.4922 - val_accuracy: 0.9087\n",
      "Epoch 91/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.4645 - accuracy: 0.9164 - val_loss: 0.4827 - val_accuracy: 0.9143\n",
      "Epoch 92/200\n",
      "57000/57000 [==============================] - 16s 274us/step - loss: 0.4519 - accuracy: 0.9184 - val_loss: 0.4732 - val_accuracy: 0.9117\n",
      "Epoch 93/200\n",
      "57000/57000 [==============================] - 16s 274us/step - loss: 0.4403 - accuracy: 0.9198 - val_loss: 0.4597 - val_accuracy: 0.9167\n",
      "Epoch 94/200\n",
      "57000/57000 [==============================] - 16s 274us/step - loss: 0.4295 - accuracy: 0.9223 - val_loss: 0.4506 - val_accuracy: 0.9170\n",
      "Epoch 95/200\n",
      "57000/57000 [==============================] - 16s 274us/step - loss: 0.4174 - accuracy: 0.9239 - val_loss: 0.4415 - val_accuracy: 0.9153\n",
      "Epoch 96/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.4065 - accuracy: 0.9260 - val_loss: 0.4314 - val_accuracy: 0.9183\n",
      "Epoch 97/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.3960 - accuracy: 0.9282 - val_loss: 0.4192 - val_accuracy: 0.9240\n",
      "Epoch 98/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.3857 - accuracy: 0.9304 - val_loss: 0.4079 - val_accuracy: 0.9253\n",
      "Epoch 99/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.3756 - accuracy: 0.9325 - val_loss: 0.3983 - val_accuracy: 0.9283\n",
      "Epoch 100/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 0.3666 - accuracy: 0.9354 - val_loss: 0.3873 - val_accuracy: 0.9307\n",
      "Epoch 101/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.3564 - accuracy: 0.9390 - val_loss: 0.3816 - val_accuracy: 0.9330\n",
      "Epoch 102/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.3473 - accuracy: 0.9418 - val_loss: 0.3683 - val_accuracy: 0.9353\n",
      "Epoch 103/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.3377 - accuracy: 0.9446 - val_loss: 0.3650 - val_accuracy: 0.9377\n",
      "Epoch 104/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.3296 - accuracy: 0.9468 - val_loss: 0.3535 - val_accuracy: 0.9390\n",
      "Epoch 105/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 0.3210 - accuracy: 0.9509 - val_loss: 0.3439 - val_accuracy: 0.9457\n",
      "Epoch 106/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.3127 - accuracy: 0.9531 - val_loss: 0.3383 - val_accuracy: 0.9493\n",
      "Epoch 107/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.3044 - accuracy: 0.9557 - val_loss: 0.3300 - val_accuracy: 0.9463\n",
      "Epoch 108/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.2971 - accuracy: 0.9582 - val_loss: 0.3214 - val_accuracy: 0.9500\n",
      "Epoch 109/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.2880 - accuracy: 0.9604 - val_loss: 0.3140 - val_accuracy: 0.9527\n",
      "Epoch 110/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.2810 - accuracy: 0.9626 - val_loss: 0.3047 - val_accuracy: 0.9520\n",
      "Epoch 111/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.2739 - accuracy: 0.9646 - val_loss: 0.3011 - val_accuracy: 0.9537\n",
      "Epoch 112/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.2659 - accuracy: 0.9658 - val_loss: 0.2963 - val_accuracy: 0.9553\n",
      "Epoch 113/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.2587 - accuracy: 0.9681 - val_loss: 0.2923 - val_accuracy: 0.9550\n",
      "Epoch 114/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 0.2514 - accuracy: 0.9690 - val_loss: 0.2800 - val_accuracy: 0.9567\n",
      "Epoch 115/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.2449 - accuracy: 0.9703 - val_loss: 0.2762 - val_accuracy: 0.9577\n",
      "Epoch 116/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 0.2379 - accuracy: 0.9714 - val_loss: 0.2690 - val_accuracy: 0.9650\n",
      "Epoch 117/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.2318 - accuracy: 0.9722 - val_loss: 0.2648 - val_accuracy: 0.9610\n",
      "Epoch 118/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.2259 - accuracy: 0.9724 - val_loss: 0.2624 - val_accuracy: 0.9590\n",
      "Epoch 119/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 0.2202 - accuracy: 0.9734 - val_loss: 0.2525 - val_accuracy: 0.9593\n",
      "Epoch 120/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.2134 - accuracy: 0.9744 - val_loss: 0.2424 - val_accuracy: 0.9653\n",
      "Epoch 121/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.2079 - accuracy: 0.9749 - val_loss: 0.2422 - val_accuracy: 0.9600\n",
      "Epoch 122/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 0.2026 - accuracy: 0.9756 - val_loss: 0.2319 - val_accuracy: 0.9643\n",
      "Epoch 123/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.1965 - accuracy: 0.9766 - val_loss: 0.2337 - val_accuracy: 0.9620\n",
      "Epoch 124/200\n",
      "57000/57000 [==============================] - 16s 278us/step - loss: 0.1905 - accuracy: 0.9774 - val_loss: 0.2272 - val_accuracy: 0.9620\n",
      "Epoch 125/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.1863 - accuracy: 0.9774 - val_loss: 0.2185 - val_accuracy: 0.9650\n",
      "Epoch 126/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.1811 - accuracy: 0.9783 - val_loss: 0.2198 - val_accuracy: 0.9627\n",
      "Epoch 127/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.1763 - accuracy: 0.9785 - val_loss: 0.2134 - val_accuracy: 0.9663\n",
      "Epoch 128/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.1708 - accuracy: 0.9791 - val_loss: 0.2073 - val_accuracy: 0.9677\n",
      "Epoch 129/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 0.1678 - accuracy: 0.9791 - val_loss: 0.1991 - val_accuracy: 0.9697\n",
      "Epoch 130/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.1624 - accuracy: 0.9799 - val_loss: 0.2021 - val_accuracy: 0.9630\n",
      "Epoch 131/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.1575 - accuracy: 0.9803 - val_loss: 0.1902 - val_accuracy: 0.9690\n",
      "Epoch 132/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.1531 - accuracy: 0.9811 - val_loss: 0.1862 - val_accuracy: 0.9703\n",
      "Epoch 133/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.1499 - accuracy: 0.9809 - val_loss: 0.1857 - val_accuracy: 0.9693\n",
      "Epoch 134/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.1458 - accuracy: 0.9817 - val_loss: 0.1840 - val_accuracy: 0.9693\n",
      "Epoch 135/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.1423 - accuracy: 0.9813 - val_loss: 0.1769 - val_accuracy: 0.9720\n",
      "Epoch 136/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.1390 - accuracy: 0.9819 - val_loss: 0.1755 - val_accuracy: 0.9713\n",
      "Epoch 137/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.1360 - accuracy: 0.9816 - val_loss: 0.1739 - val_accuracy: 0.9720\n",
      "Epoch 138/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 0.1316 - accuracy: 0.9823 - val_loss: 0.1712 - val_accuracy: 0.9713\n",
      "Epoch 139/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.1292 - accuracy: 0.9826 - val_loss: 0.1710 - val_accuracy: 0.9683\n",
      "Epoch 140/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.1258 - accuracy: 0.9827 - val_loss: 0.1635 - val_accuracy: 0.9707\n",
      "Epoch 141/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.1220 - accuracy: 0.9831 - val_loss: 0.1583 - val_accuracy: 0.9740\n",
      "Epoch 142/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.1202 - accuracy: 0.9832 - val_loss: 0.1587 - val_accuracy: 0.9730\n",
      "Epoch 143/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.1174 - accuracy: 0.9830 - val_loss: 0.1561 - val_accuracy: 0.9720\n",
      "Epoch 144/200\n",
      "57000/57000 [==============================] - 16s 278us/step - loss: 0.1138 - accuracy: 0.9836 - val_loss: 0.1609 - val_accuracy: 0.9697\n",
      "Epoch 145/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 0.1119 - accuracy: 0.9836 - val_loss: 0.1528 - val_accuracy: 0.9733\n",
      "Epoch 146/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 0.1087 - accuracy: 0.9841 - val_loss: 0.1493 - val_accuracy: 0.9713\n",
      "Epoch 147/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.1056 - accuracy: 0.9845 - val_loss: 0.1493 - val_accuracy: 0.9717\n",
      "Epoch 148/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.1040 - accuracy: 0.9841 - val_loss: 0.1452 - val_accuracy: 0.9750\n",
      "Epoch 149/200\n",
      "57000/57000 [==============================] - 16s 277us/step - loss: 0.1014 - accuracy: 0.9845 - val_loss: 0.1447 - val_accuracy: 0.9727\n",
      "Epoch 150/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.1003 - accuracy: 0.9842 - val_loss: 0.1389 - val_accuracy: 0.9747\n",
      "Epoch 151/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.0971 - accuracy: 0.9851 - val_loss: 0.1370 - val_accuracy: 0.9727\n",
      "Epoch 152/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.0954 - accuracy: 0.9851 - val_loss: 0.1345 - val_accuracy: 0.9763\n",
      "Epoch 153/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.0915 - accuracy: 0.9861 - val_loss: 0.1307 - val_accuracy: 0.9763\n",
      "Epoch 154/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.0915 - accuracy: 0.9855 - val_loss: 0.1251 - val_accuracy: 0.9770\n",
      "Epoch 155/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.0893 - accuracy: 0.9857 - val_loss: 0.1295 - val_accuracy: 0.9767\n",
      "Epoch 156/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.0880 - accuracy: 0.9858 - val_loss: 0.1286 - val_accuracy: 0.9753\n",
      "Epoch 157/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.0869 - accuracy: 0.9853 - val_loss: 0.1257 - val_accuracy: 0.9760\n",
      "Epoch 158/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.0848 - accuracy: 0.9853 - val_loss: 0.1303 - val_accuracy: 0.9757\n",
      "Epoch 159/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.0823 - accuracy: 0.9860 - val_loss: 0.1315 - val_accuracy: 0.9727\n",
      "Epoch 160/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.0814 - accuracy: 0.9857 - val_loss: 0.1227 - val_accuracy: 0.9770\n",
      "Epoch 161/200\n",
      "57000/57000 [==============================] - 16s 276us/step - loss: 0.0803 - accuracy: 0.9858 - val_loss: 0.1233 - val_accuracy: 0.9737\n",
      "Epoch 162/200\n",
      "57000/57000 [==============================] - 16s 274us/step - loss: 0.0778 - accuracy: 0.9864 - val_loss: 0.1207 - val_accuracy: 0.9733\n",
      "Epoch 163/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.0781 - accuracy: 0.9858 - val_loss: 0.1227 - val_accuracy: 0.9750\n",
      "Epoch 164/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.0771 - accuracy: 0.9858 - val_loss: 0.1174 - val_accuracy: 0.9763\n",
      "Epoch 165/200\n",
      "57000/57000 [==============================] - 16s 274us/step - loss: 0.0745 - accuracy: 0.9862 - val_loss: 0.1147 - val_accuracy: 0.9783\n",
      "Epoch 166/200\n",
      "57000/57000 [==============================] - 16s 274us/step - loss: 0.0734 - accuracy: 0.9861 - val_loss: 0.1136 - val_accuracy: 0.9763\n",
      "Epoch 167/200\n",
      "57000/57000 [==============================] - 16s 274us/step - loss: 0.0716 - accuracy: 0.9869 - val_loss: 0.1166 - val_accuracy: 0.9743\n",
      "Epoch 168/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.0708 - accuracy: 0.9865 - val_loss: 0.1166 - val_accuracy: 0.9727\n",
      "Epoch 169/200\n",
      "57000/57000 [==============================] - 16s 274us/step - loss: 0.0701 - accuracy: 0.9861 - val_loss: 0.1110 - val_accuracy: 0.9773\n",
      "Epoch 170/200\n",
      "57000/57000 [==============================] - 16s 274us/step - loss: 0.0693 - accuracy: 0.9864 - val_loss: 0.1135 - val_accuracy: 0.9743\n",
      "Epoch 171/200\n",
      "57000/57000 [==============================] - 16s 274us/step - loss: 0.0682 - accuracy: 0.9863 - val_loss: 0.1097 - val_accuracy: 0.9763\n",
      "Epoch 172/200\n",
      "57000/57000 [==============================] - 16s 273us/step - loss: 0.0667 - accuracy: 0.9866 - val_loss: 0.1055 - val_accuracy: 0.9767\n",
      "Epoch 173/200\n",
      "57000/57000 [==============================] - 16s 274us/step - loss: 0.0667 - accuracy: 0.9866 - val_loss: 0.1061 - val_accuracy: 0.9780\n",
      "Epoch 174/200\n",
      "57000/57000 [==============================] - 16s 273us/step - loss: 0.0657 - accuracy: 0.9862 - val_loss: 0.1104 - val_accuracy: 0.9750\n",
      "Epoch 175/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.0645 - accuracy: 0.9867 - val_loss: 0.1043 - val_accuracy: 0.9767\n",
      "Epoch 176/200\n",
      "57000/57000 [==============================] - 16s 274us/step - loss: 0.0642 - accuracy: 0.9862 - val_loss: 0.1067 - val_accuracy: 0.9780\n",
      "Epoch 177/200\n",
      "57000/57000 [==============================] - 16s 275us/step - loss: 0.0631 - accuracy: 0.9868 - val_loss: 0.1022 - val_accuracy: 0.9783\n",
      "Epoch 178/200\n",
      "57000/57000 [==============================] - 16s 274us/step - loss: 0.0611 - accuracy: 0.9867 - val_loss: 0.0997 - val_accuracy: 0.9783\n",
      "Epoch 179/200\n",
      "57000/57000 [==============================] - 16s 274us/step - loss: 0.0600 - accuracy: 0.9869 - val_loss: 0.0961 - val_accuracy: 0.9807\n",
      "\n",
      " Reached accuracy of 98% hence stopping training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f5ca0715be0>"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit([X_train,X_traintext],Y_train,batch_size=256,epochs=200,validation_split=0.05,callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lLWH97kI3fra"
   },
   "source": [
    "Evaluating model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "WUprKjlA9MIr",
    "outputId": "d1ad5eb0-beba-4c6e-916a-1bf3dbe1137b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5714/5714 [==============================] - 2s 405us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08105746813126281, 0.9821491241455078]"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.evaluate([X_test,X_testtext],Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seLa10dGgmXN"
   },
   "outputs": [],
   "source": [
    "final_model.save('/content/drive/My Drive/cross_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ge4mMCw-hBUp"
   },
   "outputs": [],
   "source": [
    "final_model.save_weights('/content/drive/My Drive/cross_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C2D3spPxgpzi"
   },
   "source": [
    "# **Semantic Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ybTS2rOWhuTd"
   },
   "outputs": [],
   "source": [
    "## Extracting unique classes from training dataframe\n",
    "classes=np.array(train['classes'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hj1UO8nMhuRI"
   },
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "  ## In this step we are converting charecters into lower case\n",
    "  for i in range(data.shape[0]):\n",
    "    data[i]=data[i].lower()\n",
    "  ## Replacing \"<\" with \" \" (white spaces)\n",
    "  for i in range(data.shape[0]):\n",
    "    data[i]=data[i].replace('<',\" \")\n",
    "  ## In this step we are removing numbers and puntuation marks from text\n",
    "  for i in range(data.shape[0]):\n",
    "    data[i]=re.sub('[^A-Za-z\" \"]+', '', data[i])\n",
    "  ## Here, we are removing extra space from the end of the sentence\n",
    "  for i in range(data.shape[0]):\n",
    "    data[i]=data[i].rstrip()\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "stI5BQpjipeT"
   },
   "outputs": [],
   "source": [
    "## Cleaning the classes\n",
    "classes=clean_text(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "colab_type": "code",
    "id": "stzucEMFi7wE",
    "outputId": "ca7bdb62-428a-4e6b-ea57-76e8090b274f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['beauty makeup eyeshadow', 'clothing knitwear fine knit',\n",
       "       'clothing pants straight leg', 'clothing tops blouses',\n",
       "       'shoes sandals flat', 'clothing jumpsuits full length',\n",
       "       'shoes sandals high heel', 'beauty makeup lipstick',\n",
       "       'beauty makeup foundation', 'beauty makeup lipgloss',\n",
       "       'beauty skincare moisturizer', 'shoes pumps mid heel',\n",
       "       'clothing beachwear onepiece', 'shoes sneakers low top',\n",
       "       'clothing dresses gowns', 'clothing dresses midi',\n",
       "       'clothing tops shirts', 'clothing tops tanks and camis',\n",
       "       'clothing jackets blazers', 'clothing tops tshirts',\n",
       "       'jewelry and watches fashion jewelry necklaces',\n",
       "       'shoes sandals mid heel',\n",
       "       'jewelry and watches fine jewelry necklaces',\n",
       "       'jewelry and watches fashion jewelry earrings',\n",
       "       'jewelry and watches fine jewelry rings', 'beauty skincare serum',\n",
       "       'jewelry and watches fine jewelry earrings',\n",
       "       'bags shoulder bags shoulder bags', 'bags tote bags tote bags',\n",
       "       'clothing pants wide leg', 'clothing tops sweatshirts',\n",
       "       'clothing dresses mini', 'clothing dresses maxi',\n",
       "       'jewelry and watches fine jewelry bracelets',\n",
       "       'clothing coats long', 'clothing dresses knee length',\n",
       "       'accessories wallets cardholders', 'bags shoulder bags cross body',\n",
       "       'shoes boots ankle', 'clothing jackets casual jackets',\n",
       "       'clothing skirts midi',\n",
       "       'jewelry and watches fashion jewelry bracelets',\n",
       "       'clothing knitwear medium knit', 'clothing coats knee length',\n",
       "       'shoes pumps high heel', 'lingerie sleepwear pajamas',\n",
       "       'bags tote bags mini bags', 'clothing beachwear coverups'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KXpsjgJG4HSq"
   },
   "source": [
    "We will use same word tokens here that we used for text model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nrFggqVmiCpK",
    "outputId": "3642d778-927c-44a9-e90a-7ca7a4cd0558"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (48, 10)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(classes) # Here it takes each word from classes and replaces it with it's corresponding number from our tokenizer word_index\n",
    "X = pad_sequences(X, maxlen=10) # it transforms list of sequences into numpy aray of shape (num_sequence,num_timestep) \n",
    "## In this case num_sequence=48 as we have 48 classes and num_timestep=10.\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o4GfKnk75AuQ"
   },
   "source": [
    "We will use cosine_similarity to find the similarity between the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D9aREACGjhnj"
   },
   "outputs": [],
   "source": [
    "semantic_sim=cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DN5j7LJTj1bE",
    "outputId": "a6885dce-275e-4600-d88a-ceaf452ec85d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "_RVfkC5yj3Xj",
    "outputId": "3482fe95-1171-4f0c-e1e8-65ed7ec179a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56750939, 1.        , 1.        , 0.        , 1.        ,\n",
       "       1.        , 0.99980986, 0.16981106, 0.14891433, 0.99768146,\n",
       "       0.59898778, 0.99991832, 0.        , 0.03161352, 0.        ,\n",
       "       1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "       0.99999964, 0.99999465, 0.99999964, 0.98287219, 0.99999318,\n",
       "       0.20509851, 0.98287219, 0.70710678, 0.70710678, 0.99999316,\n",
       "       0.        , 1.        , 1.        , 0.99999364, 1.        ,\n",
       "       0.        , 0.        , 0.99983232, 0.81201537, 1.        ,\n",
       "       1.        , 0.99999364, 0.9996536 , 0.        , 0.99710625,\n",
       "       0.        , 0.42288547, 0.        ])"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's see similarity of 2nd class\n",
    "semantic_sim[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xtV6pKnS5Lsu"
   },
   "source": [
    "Let's see 2nd class which is 1. Now 2nd class is one for 3rd,5th etc classes. Now look at the classes you can see 2nd class is clothing and simillarly 3rd and 5th classes are also clothing hence we are getting one at 3rd and 5th place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mwccY7XyGENc"
   },
   "source": [
    "# **Cross Modal Gap loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RDPyb5O87mv6"
   },
   "source": [
    "Now we will use output from \"concatenate_1\" layer of final model to calculate cross modal gap loss. Here I have taken reference from section \"3.4.3 Cross Modal Gap\" of given paper. I have also used some of the ideas of DeepFace and FaceNet papers here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQPChO9eGf86"
   },
   "outputs": [],
   "source": [
    "## Defining cross modal gap loss.\n",
    "def cross_modal_gap(emb):\n",
    "  emb=emb.T   ## Transpose as we want vector of shape like (1024,1), where first 512 units are \n",
    "  ##  image embeddings and last 512 units are text embeddings.\n",
    "  distance=0  ## Initializing distance to zero.\n",
    "  div=int(emb.shape[0]/2) ## Dividing emb.shape[0] which is 1024 by 2.\n",
    "  for i in range(emb.shape[1]):  ## Iterating over all the examples.\n",
    "    image_emb=emb[0:div,i]  ## Selecting first 512 units for image embeddings.\n",
    "    text_emb=emb[div:,i]    ## Selecting last 512 units for text embeddings.\n",
    "    distance +=np.mean(np.square(image_emb-text_emb)) ## Calculating the distance and adding. This distance (l2 norm) is used\n",
    "    ## in DeepFace and FaceNet paper for one shot learning and triplet loss for face recognition.\n",
    "    ## So I have taken this idea from these papers.\n",
    "  loss=(1/emb.shape[1])*distance ## Dividing by total number of examples.\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "94jX9iWl_sLD"
   },
   "source": [
    "Now we willd define a model which will give output from the 'concatenate_1' layer. Output from this model will be used for calculating cross modal gap loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rfqjrEOOw2xK"
   },
   "outputs": [],
   "source": [
    "inter_output_model = Model(final_model.input, final_model.get_layer(index = 30).output ) #index 32 is nothing but \n",
    "## the index of 'concatenate_1' layer of final_model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ee7CrkgJsi_C"
   },
   "outputs": [],
   "source": [
    "## Predicting on X_test and X_testtext datasets\n",
    "pred=inter_output_model.predict([X_test,X_testtext])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Y3Z1pWxFsvEX",
    "outputId": "eaeaf6f6-359f-4196-f653-ff21f7263203"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5714, 1024)"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cx5K5TW_AcuM"
   },
   "source": [
    "We have 5714 examples each having a vector of size 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qZkxQrXcs1zk",
    "outputId": "25c33778-1225-4564-f6f1-5142675c3cf1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0019274599617550858"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Finally let's call cross_modal_gap to get the loss.\n",
    "cross_modal_gap(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J8cci2eZhloy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "mNAvIBJ3QFa8",
    "TLSrJhczQO8D",
    "5mAxPTkiAxDd",
    "4nBpwajM2J1t",
    "rORm_ZQzVd1b",
    "inlzz2kM53A2",
    "3NJ8N_HVz7L_",
    "C2D3spPxgpzi"
   ],
   "machine_shape": "hm",
   "name": "GreenDeckMLAssignmment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
